Warning: Blocks were reordered
Section 1: Function: __do_global_dtors_aux
==========================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 1.1: Binary loops in the function named __do_global_dtors_aux
=====================================================================

Section 1.1.1: Binary loop #0
=============================

In the binary file, the address of the loop is: 4009e8

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 2: Function: bt
=======================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/bt.f

Section 2.1: Source loop ending at line 118
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 1
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 1

Section 2.1.1: Binary (requested) loop #1
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:118-118
In the binary file, the address of the loop is: 40115e

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 2.2: Source loop ending at line 127
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 2
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 2

Section 2.2.1: Binary (requested) loop #2
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:126-127
In the binary file, the address of the loop is: 401217

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.17 cycles (1.71x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.3: Source loop ending at line 141
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 3
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 3

Section 2.3.1: Binary (requested) loop #3
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:140-141
In the binary file, the address of the loop is: 401275

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.17 cycles (1.71x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4: Source loop ending at line 153
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 6
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 6

Section 2.4.1: Binary (requested) loop #6
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:145-153
In the binary file, the address of the loop is: 4012d6

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.4.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.25 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.75 cycles (1.13x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.69 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 3.75 cycles (1.47x speedup).


Section 2.5: Source loop ending at line 183
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 4
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 4

Section 2.5.1: Binary (requested) loop #4
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:182-183
In the binary file, the address of the loop is: 4015d5

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 1.25 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.50 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.6: Source loop ending at line 201
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 5
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 5

Section 2.6.1: Binary (requested) loop #5
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/bt.f:189-201
In the binary file, the address of the loop is: 4016cb

This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.6.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 42.00 to 21.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 42.00 to 39.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 26.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 26.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.4: Path #4
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 13.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 3: Function: lhsinit
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/initialize.f

Section 3.1: Source loop ending at line 212
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 7
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 7

Section 3.1.1: Binary (requested) loop #7
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:206-212
In the binary file, the address of the loop is: 401ba5

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 2.25 cycles (2.67x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 3.2: Source loop ending at line 221
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 9
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 9

Section 3.2.1: Binary (requested) loop #9
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:219-221
In the binary file, the address of the loop is: 401bf8

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



Section 3.3: Binary loops in the function named lhsinit
=======================================================

Section 3.3.1: Binary loop #8
=============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:191-212
In the binary file, the address of the loop is: 401b97

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.50 cycles (1.17x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 4: Function: initialize
===============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/initialize.f

Section 4.1: Source loop ending at line 30
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 34
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 34

Section 4.1.1: Binary (requested) loop #34
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:29-30
In the binary file, the address of the loop is: 401cb7

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.2: Source loop ending at line 75
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 30
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 30

Section 4.2.1: Binary (requested) loop #30
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:65-75
In the binary file, the address of the loop is: 401f4b

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 34% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.50 to 2.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 9.50 to 9.00 cycles (1.06x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 4.3: Source loop ending at line 97
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 26
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 26

Section 4.3.1: Binary (requested) loop #26
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:96-97
In the binary file, the address of the loop is: 4020e9

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 4.4: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 23
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 23

Section 4.4.1: Binary (requested) loop #23
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:113-114
In the binary file, the address of the loop is: 402206

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 4.5: Source loop ending at line 130
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 20
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 20

Section 4.5.1: Binary (requested) loop #20
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:129-130
In the binary file, the address of the loop is: 4022f3

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 4.6: Source loop ending at line 147
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 17
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 17

Section 4.6.1: Binary (requested) loop #17
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:146-147
In the binary file, the address of the loop is: 402413

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 4.7: Source loop ending at line 163
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 14
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 14

Section 4.7.1: Binary (requested) loop #14
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:162-163
In the binary file, the address of the loop is: 402507

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 4.8: Source loop ending at line 179
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 11
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 11

Section 4.8.1: Binary (requested) loop #11
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:178-179
In the binary file, the address of the loop is: 402628

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



Section 4.9: Binary loops in the function named initialize
==========================================================

Section 4.9.1: Binary loop #10
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:173-179
In the binary file, the address of the loop is: 402589

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.1.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.1.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 1.75 cycles (2.57x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.79 cycles (5.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 2.75 cycles (1.64x speedup).


Section 4.9.1.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.85 cycles (7.04x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 4.00 cycles (1.50x speedup).


Section 4.9.2: Binary loop #12
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:175-179
In the binary file, the address of the loop is: 4025e7

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 4.9.3: Binary loop #13
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:157-163
In the binary file, the address of the loop is: 402472

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.3.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.3.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.70 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).


Section 4.9.3.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.50 cycles (2.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.79 cycles (6.96x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 3.75 cycles (1.47x speedup).


Section 4.9.4: Binary loop #15
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:159-163
In the binary file, the address of the loop is: 4024c6

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 4.9.5: Binary loop #16
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:141-147
In the binary file, the address of the loop is: 40237e

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.5.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.5.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 1.75 cycles (2.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.67 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 4.9.5.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.50 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.76 cycles (6.91x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 4.00 cycles (1.31x speedup).


Section 4.9.6: Binary loop #18
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:143-147
In the binary file, the address of the loop is: 4023d2

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 4.9.7: Binary loop #19
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:124-130
In the binary file, the address of the loop is: 402268

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.7.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.7.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 1.50 cycles (2.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.58 cycles (5.65x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).


Section 4.9.7.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.50 cycles (1.90x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.70 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 4.9.8: Binary loop #21
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:126-130
In the binary file, the address of the loop is: 4022b2

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 4.9.9: Binary loop #22
==============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:108-114
In the binary file, the address of the loop is: 402164

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.9.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.9.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 1.75 cycles (2.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.67 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 4.9.9.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.50 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.76 cycles (6.91x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 4.00 cycles (1.31x speedup).


Section 4.9.10: Binary loop #24
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:110-114
In the binary file, the address of the loop is: 4021b8

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.00 cycles (2.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.75 cycles (7.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 4.25 cycles (1.29x speedup).


Section 4.9.11: Binary loop #25
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:91-97
In the binary file, the address of the loop is: 402051

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.11.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 4.9.11.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 1.75 cycles (1.86x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.58 cycles (5.65x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).


Section 4.9.11.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.50 cycles (1.90x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.77 cycles (6.18x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 4.9.12: Binary loop #27
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:93-97
In the binary file, the address of the loop is: 40209b

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.00 cycles (2.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.75 cycles (7.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 4.25 cycles (1.29x speedup).


Section 4.9.13: Binary loop #28
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:4-75
In the binary file, the address of the loop is: 401d1e

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.13.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.00 cycles (1.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 4.9.13.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.9.13.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.9.13.4: Path #4
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.50 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.9.13.5: Path #5
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.9.13.6: Path #6
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.50 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 4.9.14: Binary loop #29
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:4-75
In the binary file, the address of the loop is: 401d62

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.14.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 4.9.14.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.00 cycles (2.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 1.12 cycles (4.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 5.00 cycles (1.05x speedup).


Section 4.9.14.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.75 to 3.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 1.25 cycles (5.40x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.75 to 6.00 cycles (1.12x speedup).


Section 4.9.15: Binary loop #31
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:4-75
In the binary file, the address of the loop is: 401dc7

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.00 to 4.25 cycles (4.47x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.00 to 2.88 cycles (6.59x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 19.00 to 16.00 cycles (1.19x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 4.9.16: Binary loop #32
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:26-30
In the binary file, the address of the loop is: 401c6a

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.16.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 4.9.16.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 4.9.16.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 4.9.16.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 4.9.16.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 4.9.16.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 4.9.17: Binary loop #33
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:27-30
In the binary file, the address of the loop is: 401c89

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 4.9.17.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 4.9.17.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 4.9.17.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 4.9.18: Binary loop #35
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/initialize.f:28-30
In the binary file, the address of the loop is: 401ca6

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 5: Function: exact_solution
===================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/exact_solution.f

Section 5.1: Source loop ending at line 23
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 36
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 36

Section 5.1.1: Binary (requested) loop #36
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_solution.f:18-23
In the binary file, the address of the loop is: 4026b8

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.00 to 3.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 12.00 to 7.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 6: Function: exact_rhs
==============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/exact_rhs.f

Section 6.1: Source loop ending at line 26
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 73
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 73

Section 6.1.1: Binary (requested) loop #73
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:25-26
In the binary file, the address of the loop is: 402801

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.2: Source loop ending at line 45
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 68
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 68

Section 6.2.1: Binary (requested) loop #68
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:44-45
In the binary file, the address of the loop is: 4029a7

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 6.3: Source loop ending at line 51
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 69
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 69

Section 6.3.1: Binary (requested) loop #69
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:50-51
In the binary file, the address of the loop is: 4029e2

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 6.4: Source loop ending at line 93
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 67
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 67

Section 6.4.1: Binary (requested) loop #67
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:62-93
In the binary file, the address of the loop is: 402b93

10% of peak computational performance is used (1.64 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 55.00 to 49.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 55.00 to 13.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 55.00 to 41.00 cycles (1.34x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.5: Source loop ending at line 107
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 62
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 62

Section 6.5.1: Binary (requested) loop #62
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:100-107
In the binary file, the address of the loop is: 402fbf

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.6: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 65
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 65

Section 6.6.1: Binary (requested) loop #65
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:111-114
In the binary file, the address of the loop is: 403098

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.7: Source loop ending at line 125
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 63
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 63

Section 6.7.1: Binary (requested) loop #63
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:118-125
In the binary file, the address of the loop is: 403187

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.8: Source loop ending at line 144
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 58
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 58

Section 6.8.1: Binary (requested) loop #58
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:143-144
In the binary file, the address of the loop is: 4033b7

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 6.9: Source loop ending at line 150
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 59
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 59

Section 6.9.1: Binary (requested) loop #59
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:149-150
In the binary file, the address of the loop is: 4033f2

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 6.10: Source loop ending at line 191
============================================

Composition and unrolling
-------------------------
It is composed of the loop 57
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 57

Section 6.10.1: Binary (requested) loop #57
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:160-191
In the binary file, the address of the loop is: 4035a3

10% of peak computational performance is used (1.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 54.00 to 49.00 cycles (1.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 54.00 to 13.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 54.00 to 42.00 cycles (1.29x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.11: Source loop ending at line 204
============================================

Composition and unrolling
-------------------------
It is composed of the loop 52
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 52

Section 6.11.1: Binary (requested) loop #52
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:197-204
In the binary file, the address of the loop is: 4039cc

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.12: Source loop ending at line 211
============================================

Composition and unrolling
-------------------------
It is composed of the loop 55
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 55

Section 6.12.1: Binary (requested) loop #55
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:208-211
In the binary file, the address of the loop is: 403aaf

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.13: Source loop ending at line 222
============================================

Composition and unrolling
-------------------------
It is composed of the loop 53
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 53

Section 6.13.1: Binary (requested) loop #53
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:215-222
In the binary file, the address of the loop is: 403bb2

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.14: Source loop ending at line 242
============================================

Composition and unrolling
-------------------------
It is composed of the loop 48
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 48

Section 6.14.1: Binary (requested) loop #48
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:241-242
In the binary file, the address of the loop is: 403d96

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 6.15: Source loop ending at line 248
============================================

Composition and unrolling
-------------------------
It is composed of the loop 49
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 49

Section 6.15.1: Binary (requested) loop #49
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:247-248
In the binary file, the address of the loop is: 403dd1

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 6.16: Source loop ending at line 289
============================================

Composition and unrolling
-------------------------
It is composed of the loop 47
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 47

Section 6.16.1: Binary (requested) loop #47
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:258-289
In the binary file, the address of the loop is: 403fac

10% of peak computational performance is used (1.64 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 55.00 to 49.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 55.00 to 13.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 55.00 to 41.00 cycles (1.34x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.17: Source loop ending at line 302
============================================

Composition and unrolling
-------------------------
It is composed of the loop 42
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 42

Section 6.17.1: Binary (requested) loop #42
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:295-302
In the binary file, the address of the loop is: 4043ae

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.18: Source loop ending at line 309
============================================

Composition and unrolling
-------------------------
It is composed of the loop 45
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 45

Section 6.18.1: Binary (requested) loop #45
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:306-309
In the binary file, the address of the loop is: 404487

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.19: Source loop ending at line 320
============================================

Composition and unrolling
-------------------------
It is composed of the loop 43
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 43

Section 6.19.1: Binary (requested) loop #43
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:313-320
In the binary file, the address of the loop is: 404573

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.20: Source loop ending at line 333
============================================

Composition and unrolling
-------------------------
It is composed of the loop 39
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 39

Section 6.20.1: Binary (requested) loop #39
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:332-333
In the binary file, the address of the loop is: 4046b7

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 6.21: Binary loops in the function named exact_rhs
==========================================================

Section 6.21.1: Binary loop #37
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:329-333
In the binary file, the address of the loop is: 40466e

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.21.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 6.21.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 6.21.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 6.21.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 6.21.1.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 6.21.2: Binary loop #38
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:330-333
In the binary file, the address of the loop is: 404684

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.21.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 6.21.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 6.21.3: Binary loop #40
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:331-333
In the binary file, the address of the loop is: 4046a1

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 6.21.4: Binary loop #41
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-320
In the binary file, the address of the loop is: 403c9a

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 6.21.5: Binary loop #44
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-320
In the binary file, the address of the loop is: 403cfe

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 6.21.6: Binary loop #46
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-309
In the binary file, the address of the loop is: 40445b

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.7: Binary loop #50
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-255
In the binary file, the address of the loop is: 403d50

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.21.8: Binary loop #51
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-222
In the binary file, the address of the loop is: 40326d

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 6.21.9: Binary loop #54
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-222
In the binary file, the address of the loop is: 40331f

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 6.21.10: Binary loop #56
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-211
In the binary file, the address of the loop is: 403a7c

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.21.11: Binary loop #60
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-157
In the binary file, the address of the loop is: 403371

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.21.12: Binary loop #61
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-125
In the binary file, the address of the loop is: 402866

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 6.21.13: Binary loop #64
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-125
In the binary file, the address of the loop is: 40290f

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 6.21.14: Binary loop #66
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-114
In the binary file, the address of the loop is: 40306f

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.15: Binary loop #70
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:5-58
In the binary file, the address of the loop is: 402961

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.21.16: Binary loop #71
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:22-26
In the binary file, the address of the loop is: 4027b0

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.21.16.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.16.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.21.16.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.21.16.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.21.16.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.21.16.6: Path #6
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.21.17: Binary loop #72
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:23-26
In the binary file, the address of the loop is: 4027cf

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.21.17.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 6.21.17.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 6.21.17.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 6.21.18: Binary loop #74
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/exact_rhs.f:24-26
In the binary file, the address of the loop is: 4027ec

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 7: Function: compute_rhs
================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/rhs.f

Section 7.1: Source loop ending at line 33
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 130
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 130

Section 7.1.1: Binary (requested) loop #130
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-33
In the binary file, the address of the loop is: 405003

5% of peak computational performance is used (0.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 10.50 cycles (1.33x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.2: Source loop ending at line 48
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 126
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 126

Section 7.2.1: Binary (requested) loop #126
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:47-48
In the binary file, the address of the loop is: 405185

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 7.3: Source loop ending at line 110
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 123
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 123

Section 7.3.1: Binary (requested) loop #123
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-110
In the binary file, the address of the loop is: 405407

11% of peak computational performance is used (1.82 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 51.00 to 12.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 51.00 to 48.50 cycles (1.05x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.4: Source loop ending at line 122
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 113
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 113

Section 7.4.1: Binary (requested) loop #113
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:119-122
In the binary file, the address of the loop is: 4058a7

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.5: Source loop ending at line 129
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 114
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 114

Section 7.5.1: Binary (requested) loop #114
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:126-129
In the binary file, the address of the loop is: 405926

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.6: Source loop ending at line 139
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 119
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 119

Section 7.6.1: Binary (requested) loop #119
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:135-139
In the binary file, the address of the loop is: 405a1f

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.7: Source loop ending at line 149
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 115
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 115

Section 7.7.1: Binary (requested) loop #115
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:146-149
In the binary file, the address of the loop is: 405b56

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.8: Source loop ending at line 156
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 116
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 116

Section 7.8.1: Binary (requested) loop #116
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:153-156
In the binary file, the address of the loop is: 405be6

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.9: Source loop ending at line 212
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 111
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 111

Section 7.9.1: Binary (requested) loop #111
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-212
In the binary file, the address of the loop is: 405fb4

11% of peak computational performance is used (1.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 47.25 cycles (1.06x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.10: Source loop ending at line 224
============================================

Composition and unrolling
-------------------------
It is composed of the loop 106
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 106

Section 7.10.1: Binary (requested) loop #106
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:221-224
In the binary file, the address of the loop is: 406475

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.11: Source loop ending at line 233
============================================

Composition and unrolling
-------------------------
It is composed of the loop 107
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 107

Section 7.11.1: Binary (requested) loop #107
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:230-233
In the binary file, the address of the loop is: 40652b

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.12: Source loop ending at line 243
============================================

Composition and unrolling
-------------------------
It is composed of the loop 104
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 104

Section 7.12.1: Binary (requested) loop #104
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:239-243
In the binary file, the address of the loop is: 406695

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.13: Source loop ending at line 253
============================================

Composition and unrolling
-------------------------
It is composed of the loop 99
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 99

Section 7.13.1: Binary (requested) loop #99
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:250-253
In the binary file, the address of the loop is: 4067cd

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.14: Source loop ending at line 262
============================================

Composition and unrolling
-------------------------
It is composed of the loop 100
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 100

Section 7.14.1: Binary (requested) loop #100
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:259-262
In the binary file, the address of the loop is: 4068a9

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.15: Source loop ending at line 319
============================================

Composition and unrolling
-------------------------
It is composed of the loop 97
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 97

Section 7.15.1: Binary (requested) loop #97
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-319
In the binary file, the address of the loop is: 406b0a

11% of peak computational performance is used (1.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 46.75 cycles (1.07x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.16: Source loop ending at line 333
============================================

Composition and unrolling
-------------------------
It is composed of the loop 93
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 93

Section 7.16.1: Binary (requested) loop #93
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:330-333
In the binary file, the address of the loop is: 406fb9

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.17: Source loop ending at line 344
============================================

Composition and unrolling
-------------------------
It is composed of the loop 90
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 90

Section 7.17.1: Binary (requested) loop #90
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:341-344
In the binary file, the address of the loop is: 4070ad

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.18: Source loop ending at line 356
============================================

Composition and unrolling
-------------------------
It is composed of the loop 87
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 87

Section 7.18.1: Binary (requested) loop #87
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:352-356
In the binary file, the address of the loop is: 407209

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.19: Source loop ending at line 368
============================================

Composition and unrolling
-------------------------
It is composed of the loop 83
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 83

Section 7.19.1: Binary (requested) loop #83
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:365-368
In the binary file, the address of the loop is: 40735f

11% of peak computational performance is used (1.88 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 1.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.20: Source loop ending at line 379
============================================

Composition and unrolling
-------------------------
It is composed of the loop 80
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 80

Section 7.20.1: Binary (requested) loop #80
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:376-379
In the binary file, the address of the loop is: 407487

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.21: Source loop ending at line 389
============================================

Composition and unrolling
-------------------------
It is composed of the loop 77
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 77

Section 7.21.1: Binary (requested) loop #77
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:388-389
In the binary file, the address of the loop is: 407583

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 7.22: Binary loops in the function named compute_rhs
============================================================

Section 7.22.1: Binary loop #75
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:385-389
In the binary file, the address of the loop is: 40753a

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.22.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.22.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.22.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.22.1.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.22.2: Binary loop #76
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:386-389
In the binary file, the address of the loop is: 407550

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 7.22.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 7.22.3: Binary loop #78
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:387-389
In the binary file, the address of the loop is: 40756d

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 7.22.4: Binary loop #79
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-379
In the binary file, the address of the loop is: 40742e

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.4.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.4.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.25 cycles (1.11x speedup).


Section 7.22.4.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 7.22.5: Binary loop #81
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-379
In the binary file, the address of the loop is: 407461

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.6: Binary loop #82
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-368
In the binary file, the address of the loop is: 407300

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.6.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.6.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.25 cycles (1.11x speedup).


Section 7.22.6.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 7.22.7: Binary loop #84
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-368
In the binary file, the address of the loop is: 407333

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.8: Binary loop #85
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-356
In the binary file, the address of the loop is: 407163

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.8.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.8.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.8.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.8.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.50 cycles (1.14x speedup).


Section 7.22.8.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.8.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.50 cycles (1.14x speedup).


Section 7.22.9: Binary loop #86
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-356
In the binary file, the address of the loop is: 4071a0

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.9.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.9.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.22.9.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 7.22.10: Binary loop #88
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-356
In the binary file, the address of the loop is: 4071c9

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.11: Binary loop #89
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:339-344
In the binary file, the address of the loop is: 407058

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.11.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 7.22.11.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.22.11.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.75 cycles (1.18x speedup).


Section 7.22.12: Binary loop #91
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:340-344
In the binary file, the address of the loop is: 407081

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.13: Binary loop #92
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:328-333
In the binary file, the address of the loop is: 406f6f

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.13.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.13.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.22.13.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 7.22.14: Binary loop #94
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:329-333
In the binary file, the address of the loop is: 406f97

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.15: Binary loop #95
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-319
In the binary file, the address of the loop is: 406a2a

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.15.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.15.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.15.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.00 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.00 cycles (1.20x speedup).


Section 7.22.15.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.15.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.00 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.00 cycles (1.20x speedup).


Section 7.22.16: Binary loop #96
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-319
In the binary file, the address of the loop is: 406a9e

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.15 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.16.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.50 to 2.00 cycles (3.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.50 to 1.14 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.50 to 5.00 cycles (1.30x speedup).


Section 7.22.16.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.17: Binary loop #98
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-262
In the binary file, the address of the loop is: 405e63

This loop cannot be analyzed: too many paths (98 paths > MAX_NB_PATHS=8).


Section 7.22.18: Binary loop #101
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-262
In the binary file, the address of the loop is: 40687c

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.19: Binary loop #102
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-253
In the binary file, the address of the loop is: 40678d

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.20: Binary loop #103
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-243
In the binary file, the address of the loop is: 4065dc

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.20.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.20.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 0.84 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.75 to 4.75 cycles (1.42x speedup).


Section 7.22.20.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.25 to 1.03 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.25 to 6.00 cycles (1.38x speedup).


Section 7.22.21: Binary loop #105
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-243
In the binary file, the address of the loop is: 406650

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.22: Binary loop #108
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-233
In the binary file, the address of the loop is: 4064f4

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.23: Binary loop #109
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-224
In the binary file, the address of the loop is: 406442

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.24: Binary loop #110
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-212
In the binary file, the address of the loop is: 405ede

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.09 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.24.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 11.50 to 2.00 cycles (5.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.50 to 2.62 cycles (4.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.24.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.25: Binary loop #112
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-156
In the binary file, the address of the loop is: 405351

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 7.22.26: Binary loop #117
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-156
In the binary file, the address of the loop is: 405b06

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.27: Binary loop #118
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-139
In the binary file, the address of the loop is: 40599f

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.27.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 7.22.27.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.44 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.75 cycles (1.27x speedup).


Section 7.22.27.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.62 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).


Section 7.22.28: Binary loop #120
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-139
In the binary file, the address of the loop is: 4059db

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.29: Binary loop #121
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-129
In the binary file, the address of the loop is: 40585c

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 1.69 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.30: Binary loop #122
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-110
In the binary file, the address of the loop is: 4053b9

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.30.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.30.2: Path #2
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 1.50 cycles (2.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).


Section 7.22.30.3: Path #3
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.00 cycles (2.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.76 cycles (6.61x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 3.50 cycles (1.43x speedup).


Section 7.22.31: Binary loop #124
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:44-48
In the binary file, the address of the loop is: 405133

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.31.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.31.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.31.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.31.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.31.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.31.6: Path #6
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.22.32: Binary loop #125
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:45-48
In the binary file, the address of the loop is: 405152

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.32.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.22.32.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 7.22.32.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 7.22.33: Binary loop #127
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:46-48
In the binary file, the address of the loop is: 40516f

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 7.22.34: Binary loop #128
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-33
In the binary file, the address of the loop is: 404fa4

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.34.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.22.34.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.41 cycles (6.15x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.50 cycles (1.67x speedup).


Section 7.22.34.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).


Section 7.22.34.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.41 cycles (6.15x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.50 cycles (1.67x speedup).


Section 7.22.34.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).


Section 7.22.35: Binary loop #129
=================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/rhs.f:4-33
In the binary file, the address of the loop is: 404fd9

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.22.35.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 7.22.35.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 8: Function: x_solve
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/x_solve.f

Section 8.1: Source loop ending at line 128
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 138
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 138

Section 8.1.1: Binary (requested) loop #138
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-128
In the binary file, the address of the loop is: 4077ab

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 50.00 to 33.75 cycles (1.48x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 43.75 cycles (1.14x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.2: Source loop ending at line 298
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 137
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 137

Section 8.2.1: Binary (requested) loop #137
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-298
In the binary file, the address of the loop is: 407ca9

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 108.50 to 28.68 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 108.50 to 99.00 cycles (1.10x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.3: Source loop ending at line 353
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 136
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 136

Section 8.3.1: Binary (requested) loop #136
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-353
In the binary file, the address of the loop is: 40897b

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 2.19 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.75 to 7.00 cycles (1.25x speedup).


Section 8.4: Source loop ending at line 388
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 133
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 133

Section 8.4.1: Binary (requested) loop #133
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:386-388
In the binary file, the address of the loop is: 408b6c

5% of peak computational performance is used (0.89 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.50 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 8.5: Binary loops in the function named x_solve
=======================================================

Section 8.5.1: Binary loop #132
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 4076c7

This loop cannot be analyzed: too many paths (24 paths > MAX_NB_PATHS=8).


Section 8.5.2: Binary loop #134
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 408b5e

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.21 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 8.5.3: Binary loop #131
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 407654

This loop cannot be analyzed: too many paths (41 paths > MAX_NB_PATHS=8).


Section 8.5.4: Binary loop #135
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 408b26

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.50 cycles (1.07x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 9: Function: y_solve
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/y_solve.f

Section 9.1: Source loop ending at line 126
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 146
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 146

Section 9.1.1: Binary (requested) loop #146
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-126
In the binary file, the address of the loop is: 408dbe

7% of peak computational performance is used (1.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 50.00 to 34.00 cycles (1.47x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 45.00 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.2: Source loop ending at line 297
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 145
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 145

Section 9.2.1: Binary (requested) loop #145
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-297
In the binary file, the address of the loop is: 4092e1

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 108.50 to 28.68 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 108.50 to 99.00 cycles (1.10x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.3: Source loop ending at line 349
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 144
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 144

Section 9.3.1: Binary (requested) loop #144
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-349
In the binary file, the address of the loop is: 409fa9

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.50 to 2.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 10.50 to 7.00 cycles (1.50x speedup).


Section 9.4: Source loop ending at line 387
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 141
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 141

Section 9.4.1: Binary (requested) loop #141
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:385-387
In the binary file, the address of the loop is: 40a1ec

5% of peak computational performance is used (0.89 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.50 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 9.5: Binary loops in the function named y_solve
=======================================================

Section 9.5.1: Binary loop #140
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 408ceb

This loop cannot be analyzed: too many paths (24 paths > MAX_NB_PATHS=8).


Section 9.5.2: Binary loop #143
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 40a18b

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.75 to 5.00 cycles (1.15x speedup).


Section 9.5.3: Binary loop #142
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 40a1de

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.21 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 9.5.4: Binary loop #139
===============================

The loop is defined in /home/jgarcia/TFG/source_code/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 408c78

This loop cannot be analyzed: too many paths (41 paths > MAX_NB_PATHS=8).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 10: Function: z_solve
=============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/z_solve.f

Section 10.1: Source loop ending at line 126
============================================

Composition and unrolling
-------------------------
It is composed of the loop 154
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 154

Section 10.1.1: Binary (requested) loop #154
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-126
In the binary file, the address of the loop is: 40c825

7% of peak computational performance is used (1.13 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 54.00 to 35.75 cycles (1.51x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 54.00 to 13.25 cycles (4.08x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 54.00 to 45.50 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 10.2: Source loop ending at line 298
============================================

Composition and unrolling
-------------------------
It is composed of the loop 153
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 153

Section 10.2.1: Binary (requested) loop #153
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-298
In the binary file, the address of the loop is: 40cd5f

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 108.50 to 28.68 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 108.50 to 99.00 cycles (1.10x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 10.3: Source loop ending at line 356
============================================

Composition and unrolling
-------------------------
It is composed of the loop 152
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 152

Section 10.3.1: Binary (requested) loop #152
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-356
In the binary file, the address of the loop is: 40da1f

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.00 to 2.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.00 to 5.00 cycles (1.80x speedup).


Section 10.4: Source loop ending at line 400
============================================

Composition and unrolling
-------------------------
It is composed of the loop 149
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 149

Section 10.4.1: Binary (requested) loop #149
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:398-400
In the binary file, the address of the loop is: 40dc0f

5% of peak computational performance is used (0.89 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.50 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 10.5: Binary loops in the function named z_solve
========================================================

Section 10.5.1: Binary loop #147
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40c6ec

This loop cannot be analyzed: too many paths (41 paths > MAX_NB_PATHS=8).


Section 10.5.2: Binary loop #150
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40dc01

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.21 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 10.5.3: Binary loop #151
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40dbbc

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 10.5.4: Binary loop #148
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40c73c

This loop cannot be analyzed: too many paths (24 paths > MAX_NB_PATHS=8).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 11: Function: add
=========================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/add.f

Section 11.1: Source loop ending at line 22
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 157
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 157

Section 11.1.1: Binary (requested) loop #157
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/add.f:21-22
In the binary file, the address of the loop is: 40dd66

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 11.2: Binary loops in the function named add
====================================================

Section 11.2.1: Binary loop #158
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/add.f:20-22
In the binary file, the address of the loop is: 40dd54

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 11.2.2: Binary loop #155
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/add.f:18-22
In the binary file, the address of the loop is: 40dd22

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 11.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 11.2.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 11.2.2.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 11.2.2.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 11.2.2.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 11.2.3: Binary loop #156
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/add.f:19-22
In the binary file, the address of the loop is: 40dd37

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.2.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 11.2.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 11.2.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 12: Function: rhs_norm
==============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/error.f

Section 12.1: Source loop ending at line 64
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 159
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 159

Section 12.1.1: Binary (requested) loop #159
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:63-64
In the binary file, the address of the loop is: 40dde4

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 12.2: Source loop ending at line 72
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 163
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 163

Section 12.2.1: Binary (requested) loop #163
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:70-72
In the binary file, the address of the loop is: 40de7f

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.75 to 1.50 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.25 cycles (1.40x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 12.3: Source loop ending at line 82
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 160
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 160

Section 12.3.1: Binary (requested) loop #160
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:78-82
In the binary file, the address of the loop is: 40ded5

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 4.00 cycles (14.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 12.4: Binary loops in the function named rhs_norm
=========================================================

Section 12.4.1: Binary loop #162
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:68-72
In the binary file, the address of the loop is: 40de4c

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.4.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 12.4.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 12.4.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 12.4.2: Binary loop #161
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:67-72
In the binary file, the address of the loop is: 40de37

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.4.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 12.4.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 12.4.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 12.4.2.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 12.4.2.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 12.4.2.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 12.4.3: Binary loop #164
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:69-72
In the binary file, the address of the loop is: 40de69

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 13: Function: error_norm
================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/error.f

Section 13.1: Source loop ending at line 20
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 165
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 165

Section 13.1.1: Binary (requested) loop #165
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:19-20
In the binary file, the address of the loop is: 40df31

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.2: Source loop ending at line 33
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 169
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 169

Section 13.2.1: Binary (requested) loop #169
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:31-33
In the binary file, the address of the loop is: 40e051

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.3: Source loop ending at line 43
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 166
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 166

Section 13.3.1: Binary (requested) loop #166
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:39-43
In the binary file, the address of the loop is: 40e0be

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 5.00 cycles (11.20x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 13.4: Binary loops in the function named error_norm
===========================================================

Section 13.4.1: Binary loop #170
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:27-33
In the binary file, the address of the loop is: 40e00e

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.73 cycles (6.55x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 13.4.2: Binary loop #168
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:25-33
In the binary file, the address of the loop is: 40dfbc

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.4.2.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 13.4.2.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 1.50 cycles (2.83x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.76 cycles (5.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 2.50 cycles (1.70x speedup).


Section 13.4.2.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.75 to 2.50 cycles (2.30x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.75 to 0.82 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.75 to 4.00 cycles (1.44x speedup).


Section 13.4.3: Binary loop #167
================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/error.f:23-33
In the binary file, the address of the loop is: 40df64

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.4.3.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.00 cycles (1.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 13.4.3.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.3.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.3.4: Path #4
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.50 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.3.5: Path #5
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.3.6: Path #6
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.50 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 14: Function: verify
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/BT/verify.f

Section 14.1: Source loop ending at line 37
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 171
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 171

Section 14.1.1: Binary (requested) loop #171
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/verify.f:36-37
In the binary file, the address of the loop is: 40e17f

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 1.75 cycles (8.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 14.2: Source loop ending at line 46
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 172
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 172

Section 14.2.1: Binary (requested) loop #172
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/verify.f:44-46
In the binary file, the address of the loop is: 40e1b5

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.3: Source loop ending at line 274
============================================

Composition and unrolling
-------------------------
It is composed of the loop 173
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 173

Section 14.3.1: Binary (requested) loop #173
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/verify.f:271-274
In the binary file, the address of the loop is: 40e955

0% of peak computational performance is used (0.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 32% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 4.50 cycles (6.22x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 14.4: Source loop ending at line 315
============================================

Composition and unrolling
-------------------------
It is composed of the loop 174
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 174

Section 14.4.1: Binary (requested) loop #174
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/verify.f:308-315
In the binary file, the address of the loop is: 40ec91

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 14.4.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 11% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.4.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 3.50 cycles (3.71x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 11.75 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.4.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 3.50 cycles (4.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.5: Source loop ending at line 334
============================================

Composition and unrolling
-------------------------
It is composed of the loop 175
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 175

Section 14.5.1: Binary (requested) loop #175
============================================

The loop is defined in /home/jgarcia/TFG/source_code/BT/verify.f:327-334
In the binary file, the address of the loop is: 40efd2

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 14.5.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 11% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.5.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 3.50 cycles (3.71x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 11.75 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.5.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 3.50 cycles (4.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Section 15: Function: __libc_csu_init
=====================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 15.1: Binary loops in the function named __libc_csu_init
================================================================

Section 15.1.1: Binary loop #176
================================

In the binary file, the address of the loop is: 40ffc0

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.00 cycles (2.00x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Section 16: Function: __do_global_ctors_aux
===========================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 16.1: Binary loops in the function named __do_global_ctors_aux
======================================================================

Section 16.1.1: Binary loop #177
================================

In the binary file, the address of the loop is: 410020

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



All innermost loops were analyzed.

