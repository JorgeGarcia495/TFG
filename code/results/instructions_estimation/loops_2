Warning: Blocks were reordered
Section 1: Function: __do_global_dtors_aux
==========================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 1.1: Binary loops in the function named __do_global_dtors_aux
=====================================================================

Section 1.1.1: Binary loop #0
=============================

In the binary file, the address of the loop is: 4009e8

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 2: Function: sp
=======================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/sp.f

Section 2.1: Source loop ending at line 119
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 1
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 1

Section 2.1.1: Binary (requested) loop #1
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:119-119
In the binary file, the address of the loop is: 4011c1

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 2.2: Source loop ending at line 130
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 2
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 2

Section 2.2.1: Binary (requested) loop #2
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:129-130
In the binary file, the address of the loop is: 4012a1

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.17 cycles (1.71x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.3: Source loop ending at line 144
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 3
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 3

Section 2.3.1: Binary (requested) loop #3
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:143-144
In the binary file, the address of the loop is: 4012ff

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.17 cycles (1.71x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4: Source loop ending at line 155
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 6
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 6

Section 2.4.1: Binary (requested) loop #6
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:148-155
In the binary file, the address of the loop is: 401360

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.4.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.25 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.75 cycles (1.13x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.69 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 3.75 cycles (1.47x speedup).


Section 2.5: Source loop ending at line 187
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 4
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 4

Section 2.5.1: Binary (requested) loop #4
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:186-187
In the binary file, the address of the loop is: 401671

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 1.25 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.50 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.6: Source loop ending at line 205
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 5
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 5

Section 2.6.1: Binary (requested) loop #5
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/sp.f:193-205
In the binary file, the address of the loop is: 401767

This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.6.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 42.00 to 21.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 42.00 to 39.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 26.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 26.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.6.1.4: Path #4
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 13.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 3: Function: lhsinit
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/initialize.f

Section 3.1: Source loop ending at line 215
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 7
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 7

Section 3.1.1: Binary (requested) loop #7
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:209-215
In the binary file, the address of the loop is: 401c66

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 2.25 cycles (2.67x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



Section 3.2: Binary loops in the function named lhsinit
=======================================================

Section 3.2.1: Binary loop #8
=============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:196-222
In the binary file, the address of the loop is: 401c3e

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 4.75 cycles (1.26x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 4: Function: lhsinitj
=============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/initialize.f

Section 4.1: Source loop ending at line 248
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 9
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 9

Section 4.1.1: Binary (requested) loop #9
=========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:242-248
In the binary file, the address of the loop is: 401d38

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 2.25 cycles (2.67x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



Section 4.2: Binary loops in the function named lhsinitj
========================================================

Section 4.2.1: Binary loop #10
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:229-255
In the binary file, the address of the loop is: 401d1a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 4.00 cycles (1.50x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 5: Function: initialize
===============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/initialize.f

Section 5.1: Source loop ending at line 34
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 35
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 35

Section 5.1.1: Binary (requested) loop #35
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-34
In the binary file, the address of the loop is: 401e55

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.25 cycles (1.18x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.2: Source loop ending at line 77
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 31
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 31

Section 5.2.1: Binary (requested) loop #31
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:67-77
In the binary file, the address of the loop is: 40211c

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.50 to 2.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 9.50 to 8.25 cycles (1.15x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.3: Source loop ending at line 101
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 27
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 27

Section 5.3.1: Binary (requested) loop #27
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:100-101
In the binary file, the address of the loop is: 4022c5

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 5.4: Source loop ending at line 118
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 24
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 24

Section 5.4.1: Binary (requested) loop #24
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:117-118
In the binary file, the address of the loop is: 4023e2

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 5.5: Source loop ending at line 135
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 21
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 21

Section 5.5.1: Binary (requested) loop #21
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:134-135
In the binary file, the address of the loop is: 4024cf

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 5.6: Source loop ending at line 153
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 18
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 18

Section 5.6.1: Binary (requested) loop #18
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:152-153
In the binary file, the address of the loop is: 4025ef

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 5.7: Source loop ending at line 170
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 15
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 15

Section 5.7.1: Binary (requested) loop #15
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:169-170
In the binary file, the address of the loop is: 4026e3

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 5.8: Source loop ending at line 187
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 12
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 12

Section 5.8.1: Binary (requested) loop #12
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:186-187
In the binary file, the address of the loop is: 402804

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



Section 5.9: Binary loops in the function named initialize
==========================================================

Section 5.9.1: Binary loop #11
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:181-187
In the binary file, the address of the loop is: 402765

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.1.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.1.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 1.75 cycles (2.57x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.79 cycles (5.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 2.75 cycles (1.64x speedup).


Section 5.9.1.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.85 cycles (7.04x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 4.00 cycles (1.50x speedup).


Section 5.9.2: Binary loop #13
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:183-187
In the binary file, the address of the loop is: 4027c3

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 5.9.3: Binary loop #14
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:164-170
In the binary file, the address of the loop is: 40264e

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.3.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.3.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.70 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).


Section 5.9.3.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.50 cycles (2.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.79 cycles (6.96x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 3.75 cycles (1.47x speedup).


Section 5.9.4: Binary loop #16
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:166-170
In the binary file, the address of the loop is: 4026a2

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 5.9.5: Binary loop #17
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:147-153
In the binary file, the address of the loop is: 40255a

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.5.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.5.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 1.75 cycles (2.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.67 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 5.9.5.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.50 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.76 cycles (6.91x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 4.00 cycles (1.31x speedup).


Section 5.9.6: Binary loop #19
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:149-153
In the binary file, the address of the loop is: 4025ae

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 5.9.7: Binary loop #20
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:129-135
In the binary file, the address of the loop is: 402444

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.7.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.7.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 1.50 cycles (2.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.58 cycles (5.65x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).


Section 5.9.7.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.50 cycles (1.90x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.70 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 5.9.8: Binary loop #22
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:131-135
In the binary file, the address of the loop is: 40248e

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.66 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).


Section 5.9.9: Binary loop #23
==============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:112-118
In the binary file, the address of the loop is: 402340

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.9.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.9.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 1.75 cycles (2.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.67 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 5.9.9.3: Path #3
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.50 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.76 cycles (6.91x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 4.00 cycles (1.31x speedup).


Section 5.9.10: Binary loop #25
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:114-118
In the binary file, the address of the loop is: 402394

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.00 cycles (2.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.75 cycles (7.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 4.25 cycles (1.29x speedup).


Section 5.9.11: Binary loop #26
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:95-101
In the binary file, the address of the loop is: 40222d

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.11.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 5.9.11.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 1.75 cycles (1.86x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.58 cycles (5.65x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).


Section 5.9.11.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.50 cycles (1.90x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.77 cycles (6.18x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 5.9.12: Binary loop #28
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:97-101
In the binary file, the address of the loop is: 402277

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.00 cycles (2.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.75 cycles (7.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 4.25 cycles (1.29x speedup).


Section 5.9.13: Binary loop #29
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-77
In the binary file, the address of the loop is: 401eee

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.13.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.00 cycles (1.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 5.9.13.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.13.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.13.4: Path #4
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.50 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.13.5: Path #5
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 1.50 cycles (2.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.13.6: Path #6
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.50 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.14: Binary loop #30
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-77
In the binary file, the address of the loop is: 401f32

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.14.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 5.9.14.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.00 cycles (2.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 1.12 cycles (4.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 5.00 cycles (1.05x speedup).


Section 5.9.14.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.75 to 3.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 1.25 cycles (5.40x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.75 to 6.00 cycles (1.12x speedup).


Section 5.9.15: Binary loop #32
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-77
In the binary file, the address of the loop is: 401f97

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.20 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.75 to 5.00 cycles (3.95x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.75 to 3.25 cycles (6.08x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.9.16: Binary loop #33
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-34
In the binary file, the address of the loop is: 401e00

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.16.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.9.16.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.34 cycles (7.27x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.50 cycles (1.67x speedup).


Section 5.9.16.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).


Section 5.9.16.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.34 cycles (7.27x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.50 cycles (1.67x speedup).


Section 5.9.16.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).


Section 5.9.17: Binary loop #34
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/initialize.f:5-34
In the binary file, the address of the loop is: 401e31

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.9.17.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 2.75 cycles (1.18x speedup).


Section 5.9.17.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 6: Function: exact_solution
===================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/exact_solution.f

Section 6.1: Source loop ending at line 24
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 36
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 36

Section 6.1.1: Binary (requested) loop #36
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_solution.f:19-24
In the binary file, the address of the loop is: 402894

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.00 to 3.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 12.00 to 7.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 7: Function: exact_rhs
==============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/exact_rhs.f

Section 7.1: Source loop ending at line 27
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 73
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 73

Section 7.1.1: Binary (requested) loop #73
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:26-27
In the binary file, the address of the loop is: 4029e1

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 7.2: Source loop ending at line 46
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 68
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 68

Section 7.2.1: Binary (requested) loop #68
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:45-46
In the binary file, the address of the loop is: 402b8a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 7.3: Source loop ending at line 52
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 69
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 69

Section 7.3.1: Binary (requested) loop #69
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:51-52
In the binary file, the address of the loop is: 402bc5

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 7.4: Source loop ending at line 94
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 67
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 67

Section 7.4.1: Binary (requested) loop #67
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:63-94
In the binary file, the address of the loop is: 402d7a

10% of peak computational performance is used (1.64 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 55.00 to 49.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 55.00 to 13.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 55.00 to 41.00 cycles (1.34x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.5: Source loop ending at line 107
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 62
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 62

Section 7.5.1: Binary (requested) loop #62
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:100-107
In the binary file, the address of the loop is: 4031b5

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.6: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 66
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 66

Section 7.6.1: Binary (requested) loop #66
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:111-114
In the binary file, the address of the loop is: 40327a

5% of peak computational performance is used (0.82 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 11.00 to 5.25 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 2.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.75 cycles (1.42x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.7: Source loop ending at line 125
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 64
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 64

Section 7.7.1: Binary (requested) loop #64
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:118-125
In the binary file, the address of the loop is: 4033c5

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.8: Source loop ending at line 144
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 58
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 58

Section 7.8.1: Binary (requested) loop #58
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:143-144
In the binary file, the address of the loop is: 4035f7

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 7.9: Source loop ending at line 149
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 59
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 59

Section 7.9.1: Binary (requested) loop #59
==========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:148-149
In the binary file, the address of the loop is: 403632

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 7.10: Source loop ending at line 190
============================================

Composition and unrolling
-------------------------
It is composed of the loop 57
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 57

Section 7.10.1: Binary (requested) loop #57
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:159-190
In the binary file, the address of the loop is: 4037e2

10% of peak computational performance is used (1.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 54.00 to 49.00 cycles (1.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 54.00 to 13.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 54.00 to 42.00 cycles (1.29x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.11: Source loop ending at line 203
============================================

Composition and unrolling
-------------------------
It is composed of the loop 52
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 52

Section 7.11.1: Binary (requested) loop #52
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:196-203
In the binary file, the address of the loop is: 403c12

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.12: Source loop ending at line 210
============================================

Composition and unrolling
-------------------------
It is composed of the loop 56
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 56

Section 7.12.1: Binary (requested) loop #56
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:207-210
In the binary file, the address of the loop is: 403cd7

5% of peak computational performance is used (0.82 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 11.00 to 5.25 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 2.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 8.50 cycles (1.29x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.13: Source loop ending at line 221
============================================

Composition and unrolling
-------------------------
It is composed of the loop 54
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 54

Section 7.13.1: Binary (requested) loop #54
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:214-221
In the binary file, the address of the loop is: 403e40

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.14: Source loop ending at line 241
============================================

Composition and unrolling
-------------------------
It is composed of the loop 48
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 48

Section 7.14.1: Binary (requested) loop #48
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:240-241
In the binary file, the address of the loop is: 404024

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 7.15: Source loop ending at line 247
============================================

Composition and unrolling
-------------------------
It is composed of the loop 49
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 49

Section 7.15.1: Binary (requested) loop #49
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:246-247
In the binary file, the address of the loop is: 40405f

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 7.16: Source loop ending at line 288
============================================

Composition and unrolling
-------------------------
It is composed of the loop 47
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 47

Section 7.16.1: Binary (requested) loop #47
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:257-288
In the binary file, the address of the loop is: 40422d

10% of peak computational performance is used (1.64 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 55.00 to 49.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 55.00 to 13.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 55.00 to 41.00 cycles (1.34x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.17: Source loop ending at line 301
============================================

Composition and unrolling
-------------------------
It is composed of the loop 42
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 42

Section 7.17.1: Binary (requested) loop #42
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:294-301
In the binary file, the address of the loop is: 40462d

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.18: Source loop ending at line 308
============================================

Composition and unrolling
-------------------------
It is composed of the loop 46
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 46

Section 7.18.1: Binary (requested) loop #46
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:305-308
In the binary file, the address of the loop is: 4046f2

4% of peak computational performance is used (0.75 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 12.00 to 5.25 cycles (2.29x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.00 to 3.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 12.00 to 7.75 cycles (1.55x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.19: Source loop ending at line 319
============================================

Composition and unrolling
-------------------------
It is composed of the loop 44
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 44

Section 7.19.1: Binary (requested) loop #44
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:312-319
In the binary file, the address of the loop is: 40483e

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 4.50 cycles (1.56x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.20: Source loop ending at line 332
============================================

Composition and unrolling
-------------------------
It is composed of the loop 39
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 39

Section 7.20.1: Binary (requested) loop #39
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:331-332
In the binary file, the address of the loop is: 404984

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 7.21: Binary loops in the function named exact_rhs
==========================================================

Section 7.21.1: Binary loop #37
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:328-332
In the binary file, the address of the loop is: 40493b

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.21.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.21.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.21.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.21.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 7.21.1.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 7.21.2: Binary loop #38
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:329-332
In the binary file, the address of the loop is: 404951

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.21.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 7.21.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 7.21.3: Binary loop #40
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:330-332
In the binary file, the address of the loop is: 40496e

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 7.21.4: Binary loop #41
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-319
In the binary file, the address of the loop is: 403f28

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 7.21.5: Binary loop #43
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:304-308
In the binary file, the address of the loop is: 4046dd

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.5.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.28 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).


Section 7.21.5.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.19 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.6: Binary loop #45
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-319
In the binary file, the address of the loop is: 403f8c

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 7.21.7: Binary loop #50
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-254
In the binary file, the address of the loop is: 403fde

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.21.8: Binary loop #51
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-221
In the binary file, the address of the loop is: 4034ad

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 7.21.9: Binary loop #53
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:206-210
In the binary file, the address of the loop is: 403cc2

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.9.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.28 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).


Section 7.21.9.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.19 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.10: Binary loop #55
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-221
In the binary file, the address of the loop is: 40355f

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 7.21.11: Binary loop #60
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-156
In the binary file, the address of the loop is: 4035b1

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.21.12: Binary loop #61
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-125
In the binary file, the address of the loop is: 402a46

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 7.21.13: Binary loop #63
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:110-114
In the binary file, the address of the loop is: 403265

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.13.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.28 cycles (7.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).


Section 7.21.13.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.19 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.14: Binary loop #65
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-125
In the binary file, the address of the loop is: 402af2

This loop cannot be analyzed: too many paths (11 paths > MAX_NB_PATHS=8).


Section 7.21.15: Binary loop #70
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:5-59
In the binary file, the address of the loop is: 402b44

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.25 cycles (1.24x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.21.16: Binary loop #71
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:23-27
In the binary file, the address of the loop is: 402990

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.16.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.21.16.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.16.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.16.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.16.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.16.6: Path #6
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.21.17: Binary loop #72
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:24-27
In the binary file, the address of the loop is: 4029af

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 7.21.17.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 7.21.17.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 7.21.17.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 7.21.18: Binary loop #74
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/exact_rhs.f:25-27
In the binary file, the address of the loop is: 4029cc

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 8: Function: compute_rhs
================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/rhs.f

Section 8.1: Source loop ending at line 39
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 130
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 130

Section 8.1.1: Binary (requested) loop #130
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-39
In the binary file, the address of the loop is: 40531d

3% of peak computational performance is used (0.57 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 13.00 cycles (2.15x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.2: Source loop ending at line 54
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 126
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 126

Section 8.2.1: Binary (requested) loop #126
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:53-54
In the binary file, the address of the loop is: 4054d4

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 8.3: Source loop ending at line 115
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 123
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 123

Section 8.3.1: Binary (requested) loop #123
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-115
In the binary file, the address of the loop is: 40575a

11% of peak computational performance is used (1.82 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 51.00 to 12.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 51.00 to 48.50 cycles (1.05x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.4: Source loop ending at line 128
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 113
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 113

Section 8.4.1: Binary (requested) loop #113
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:125-128
In the binary file, the address of the loop is: 405bfa

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.5: Source loop ending at line 135
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 114
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 114

Section 8.5.1: Binary (requested) loop #114
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:132-135
In the binary file, the address of the loop is: 405c79

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.6: Source loop ending at line 145
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 119
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 119

Section 8.6.1: Binary (requested) loop #119
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:141-145
In the binary file, the address of the loop is: 405d72

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.7: Source loop ending at line 155
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 115
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 115

Section 8.7.1: Binary (requested) loop #115
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:152-155
In the binary file, the address of the loop is: 405ea9

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.8: Source loop ending at line 162
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 116
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 116

Section 8.8.1: Binary (requested) loop #116
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:159-162
In the binary file, the address of the loop is: 405f39

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.9: Source loop ending at line 218
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 111
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 111

Section 8.9.1: Binary (requested) loop #111
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-218
In the binary file, the address of the loop is: 406308

11% of peak computational performance is used (1.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 47.25 cycles (1.06x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.10: Source loop ending at line 231
============================================

Composition and unrolling
-------------------------
It is composed of the loop 106
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 106

Section 8.10.1: Binary (requested) loop #106
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:228-231
In the binary file, the address of the loop is: 4067c9

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.11: Source loop ending at line 240
============================================

Composition and unrolling
-------------------------
It is composed of the loop 107
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 107

Section 8.11.1: Binary (requested) loop #107
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:237-240
In the binary file, the address of the loop is: 40687f

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.12: Source loop ending at line 250
============================================

Composition and unrolling
-------------------------
It is composed of the loop 104
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 104

Section 8.12.1: Binary (requested) loop #104
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:246-250
In the binary file, the address of the loop is: 4069e9

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.13: Source loop ending at line 260
============================================

Composition and unrolling
-------------------------
It is composed of the loop 99
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 99

Section 8.13.1: Binary (requested) loop #99
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:257-260
In the binary file, the address of the loop is: 406b21

11% of peak computational performance is used (1.78 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.14: Source loop ending at line 269
============================================

Composition and unrolling
-------------------------
It is composed of the loop 100
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 100

Section 8.14.1: Binary (requested) loop #100
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:266-269
In the binary file, the address of the loop is: 406bfd

10% of peak computational performance is used (1.71 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.15: Source loop ending at line 326
============================================

Composition and unrolling
-------------------------
It is composed of the loop 97
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 97

Section 8.15.1: Binary (requested) loop #97
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-326
In the binary file, the address of the loop is: 406e5c

11% of peak computational performance is used (1.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 46.75 cycles (1.07x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.16: Source loop ending at line 341
============================================

Composition and unrolling
-------------------------
It is composed of the loop 93
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 93

Section 8.16.1: Binary (requested) loop #93
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:338-341
In the binary file, the address of the loop is: 407304

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.17: Source loop ending at line 352
============================================

Composition and unrolling
-------------------------
It is composed of the loop 90
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 90

Section 8.17.1: Binary (requested) loop #90
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:349-352
In the binary file, the address of the loop is: 4073f0

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.18: Source loop ending at line 364
============================================

Composition and unrolling
-------------------------
It is composed of the loop 87
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 87

Section 8.18.1: Binary (requested) loop #87
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:360-364
In the binary file, the address of the loop is: 407543

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.19: Source loop ending at line 376
============================================

Composition and unrolling
-------------------------
It is composed of the loop 83
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 83

Section 8.19.1: Binary (requested) loop #83
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:373-376
In the binary file, the address of the loop is: 407692

11% of peak computational performance is used (1.88 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 1.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.20: Source loop ending at line 387
============================================

Composition and unrolling
-------------------------
It is composed of the loop 80
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 80

Section 8.20.1: Binary (requested) loop #80
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:384-387
In the binary file, the address of the loop is: 4077af

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.21: Source loop ending at line 397
============================================

Composition and unrolling
-------------------------
It is composed of the loop 77
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 77

Section 8.21.1: Binary (requested) loop #77
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:396-397
In the binary file, the address of the loop is: 4078a0

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 8.22: Binary loops in the function named compute_rhs
============================================================

Section 8.22.1: Binary loop #75
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:393-397
In the binary file, the address of the loop is: 407857

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 8.22.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 8.22.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 8.22.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 8.22.1.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 8.22.2: Binary loop #76
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:394-397
In the binary file, the address of the loop is: 40786d

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 8.22.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 8.22.3: Binary loop #78
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:395-397
In the binary file, the address of the loop is: 40788a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 8.22.4: Binary loop #79
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-387
In the binary file, the address of the loop is: 407756

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.4.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.4.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.25 cycles (1.11x speedup).


Section 8.22.4.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 8.22.5: Binary loop #81
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-387
In the binary file, the address of the loop is: 407789

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.6: Binary loop #82
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-376
In the binary file, the address of the loop is: 407633

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.6.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.6.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.31 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.25 cycles (1.11x speedup).


Section 8.22.6.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 8.22.7: Binary loop #84
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-376
In the binary file, the address of the loop is: 407666

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.8: Binary loop #85
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-364
In the binary file, the address of the loop is: 40749d

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.8.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.8.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.8.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.8.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.50 cycles (1.14x speedup).


Section 8.22.8.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.8.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.50 cycles (1.14x speedup).


Section 8.22.9: Binary loop #86
===============================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-364
In the binary file, the address of the loop is: 4074da

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.9.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.9.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 8.22.9.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 8.22.10: Binary loop #88
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-364
In the binary file, the address of the loop is: 407503

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.11: Binary loop #89
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:347-352
In the binary file, the address of the loop is: 40739b

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.11.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.11.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 8.22.11.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 8.22.12: Binary loop #91
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:348-352
In the binary file, the address of the loop is: 4073c4

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.13: Binary loop #92
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:336-341
In the binary file, the address of the loop is: 4072ba

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.13.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.13.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 8.22.13.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 8.22.14: Binary loop #94
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:337-341
In the binary file, the address of the loop is: 4072e2

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.15: Binary loop #95
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-326
In the binary file, the address of the loop is: 406d7c

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.15.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.15.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.15.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.00 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.00 cycles (1.20x speedup).


Section 8.22.15.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.15.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.00 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.00 cycles (1.20x speedup).


Section 8.22.16: Binary loop #96
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-326
In the binary file, the address of the loop is: 406df0

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.15 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.16.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.50 to 2.00 cycles (3.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.50 to 1.14 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.50 to 5.00 cycles (1.30x speedup).


Section 8.22.16.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.17: Binary loop #98
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-269
In the binary file, the address of the loop is: 4061b7

This loop cannot be analyzed: too many paths (98 paths > MAX_NB_PATHS=8).


Section 8.22.18: Binary loop #101
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-269
In the binary file, the address of the loop is: 406bd0

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.19: Binary loop #102
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-260
In the binary file, the address of the loop is: 406ae1

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.20: Binary loop #103
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-250
In the binary file, the address of the loop is: 406930

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.20.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.20.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 0.84 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.75 to 4.75 cycles (1.42x speedup).


Section 8.22.20.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.25 to 1.03 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.25 to 6.00 cycles (1.38x speedup).


Section 8.22.21: Binary loop #105
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-250
In the binary file, the address of the loop is: 4069a4

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.22: Binary loop #108
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-240
In the binary file, the address of the loop is: 406848

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.23: Binary loop #109
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-231
In the binary file, the address of the loop is: 406796

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 1.12 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.24: Binary loop #110
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-218
In the binary file, the address of the loop is: 406232

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.09 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.24.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 11.50 to 2.00 cycles (5.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.50 to 2.62 cycles (4.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.24.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.25: Binary loop #112
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-162
In the binary file, the address of the loop is: 4056a4

This loop cannot be analyzed: too many paths (20 paths > MAX_NB_PATHS=8).


Section 8.22.26: Binary loop #117
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-162
In the binary file, the address of the loop is: 405e59

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.27: Binary loop #118
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-145
In the binary file, the address of the loop is: 405cf2

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.27.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 8.22.27.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.44 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.75 cycles (1.27x speedup).


Section 8.22.27.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.62 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).


Section 8.22.28: Binary loop #120
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-145
In the binary file, the address of the loop is: 405d2e

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.29: Binary loop #121
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-135
In the binary file, the address of the loop is: 405baf

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 1.69 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.30: Binary loop #122
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-115
In the binary file, the address of the loop is: 40570c

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.30.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.30.2: Path #2
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 1.50 cycles (2.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).


Section 8.22.30.3: Path #3
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.00 cycles (2.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.76 cycles (6.61x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 3.50 cycles (1.43x speedup).


Section 8.22.31: Binary loop #124
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:50-54
In the binary file, the address of the loop is: 405482

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.31.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.31.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.31.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.31.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.31.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.28 cycles (6.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.31.6: Path #6
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.22.32: Binary loop #125
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:51-54
In the binary file, the address of the loop is: 4054a1

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.32.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 8.22.32.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 8.22.32.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 8.22.33: Binary loop #127
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:52-54
In the binary file, the address of the loop is: 4054be

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 8.22.34: Binary loop #128
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-39
In the binary file, the address of the loop is: 40529e

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.34.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.34.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.88 cycles (4.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.34.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.34.4: Path #4
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.88 cycles (4.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.34.5: Path #5
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 8.22.35: Binary loop #129
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/rhs.f:4-39
In the binary file, the address of the loop is: 4052e7

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 8.22.35.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.56 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.50 cycles (1.29x speedup).


Section 8.22.35.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 9: Function: x_solve
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/x_solve.f

Section 9.1: Source loop ending at line 44
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 155

The analysis will be displayed for the requested loops: 155

Section 9.1.1: Binary (requested) loop #155
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:38-44
In the binary file, the address of the loop is: 407a5a

This loop cannot be analyzed: too many paths (27 paths > MAX_NB_PATHS=8).


Section 9.2: Source loop ending at line 52
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 154
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 154

Section 9.2.1: Binary (requested) loop #154
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:47-52
In the binary file, the address of the loop is: 407b1c

6% of peak computational performance is used (1.10 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 7.25 to 5.50 cycles (1.32x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.25 to 1.39 cycles (5.22x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.25 to 5.00 cycles (1.45x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.3: Source loop ending at line 69
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 132
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 132

Section 9.3.1: Binary (requested) loop #132
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-69
In the binary file, the address of the loop is: 407bf9

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 7.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 2.19 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.75 to 7.00 cycles (1.25x speedup).


Section 9.4: Source loop ending at line 78
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 152
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 152

Section 9.4.1: Binary (requested) loop #152
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-78
In the binary file, the address of the loop is: 407cfb

4% of peak computational performance is used (0.77 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.50 to 5.00 cycles (1.30x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.50 to 1.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.50 to 6.00 cycles (1.08x speedup).


Section 9.5: Source loop ending at line 92
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 133
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 133

Section 9.5.1: Binary (requested) loop #133
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-92
In the binary file, the address of the loop is: 407dc8

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 7.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 2.19 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.75 to 7.00 cycles (1.25x speedup).


Section 9.6: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 150
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 150

Section 9.6.1: Binary (requested) loop #150
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:100-114
In the binary file, the address of the loop is: 407ecd

3% of peak computational performance is used (0.57 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 10.50 to 5.00 cycles (2.10x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.50 to 2.50 cycles (4.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 10.50 to 10.00 cycles (1.05x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.7: Source loop ending at line 142
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 146
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 146

Section 9.7.1: Binary (requested) loop #146
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:140-142
In the binary file, the address of the loop is: 408132

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.8: Source loop ending at line 150
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 147
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 147

Section 9.8.1: Binary (requested) loop #147
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:148-150
In the binary file, the address of the loop is: 4081b6

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.9: Source loop ending at line 176
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 134
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 134

Section 9.9.1: Binary (requested) loop #134
===========================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:174-176
In the binary file, the address of the loop is: 40833c

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.10: Source loop ending at line 183
============================================

Composition and unrolling
-------------------------
It is composed of the loop 135
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 135

Section 9.10.1: Binary (requested) loop #135
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:182-183
In the binary file, the address of the loop is: 408367

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 9.11: Source loop ending at line 228
============================================

Composition and unrolling
-------------------------
It is composed of the loop 143
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 143

Section 9.11.1: Binary (requested) loop #143
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:192-228
In the binary file, the address of the loop is: 408400

6% of peak computational performance is used (1.08 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 29.50 to 28.00 cycles (1.05x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 29.50 to 14.00 cycles (2.11x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.12: Source loop ending at line 264
============================================

Composition and unrolling
-------------------------
It is composed of the loop 136
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 136

Section 9.12.1: Binary (requested) loop #136
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-264
In the binary file, the address of the loop is: 4086fd

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 20.25 cycles (2.77x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.13: Source loop ending at line 278
============================================

Composition and unrolling
-------------------------
It is composed of the loop 137
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 137

Section 9.13.1: Binary (requested) loop #137
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:276-278
In the binary file, the address of the loop is: 408981

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.14: Source loop ending at line 297
============================================

Composition and unrolling
-------------------------
It is composed of the loop 139
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 139

Section 9.14.1: Binary (requested) loop #139
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:294-297
In the binary file, the address of the loop is: 408b1b

8% of peak computational performance is used (1.33 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 9.15: Binary loops in the function named x_solve
========================================================

Section 9.15.1: Binary loop #153
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:37-52
In the binary file, the address of the loop is: 407a2d

This loop cannot be analyzed: too many paths (83 paths > MAX_NB_PATHS=8).


Section 9.15.2: Binary loop #151
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-78
In the binary file, the address of the loop is: 407cda

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 9.15.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 9.15.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 9.15.3: Binary loop #138
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-308
In the binary file, the address of the loop is: 408a4b

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 9.15.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 9.15.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.38 cycles (7.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).


Section 9.15.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.56 cycles (7.53x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 3.50 cycles (1.21x speedup).


Section 9.15.4: Binary loop #149
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:99-114
In the binary file, the address of the loop is: 407e99

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 9.15.4.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.47 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.50 cycles (1.07x speedup).


Section 9.15.4.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 9.15.5: Binary loop #148
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-150
In the binary file, the address of the loop is: 408042

4% of peak computational performance is used (0.72 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.50 to 14.00 cycles (1.39x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.50 to 7.00 cycles (2.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 19.50 to 14.00 cycles (1.39x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.15.6: Binary loop #140
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-308
In the binary file, the address of the loop is: 408a7a

2% of peak computational performance is used (0.48 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 16.75 to 8.75 cycles (1.91x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 16.75 to 4.19 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 16.75 to 15.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.15.7: Binary loop #141
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-284
In the binary file, the address of the loop is: 408922

2% of peak computational performance is used (0.41 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 9.75 to 4.00 cycles (2.44x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.75 to 2.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.75 to 7.50 cycles (1.30x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.15.8: Binary loop #142
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:191-228
In the binary file, the address of the loop is: 4083d2

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 9.15.8.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 9.15.8.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.34 cycles (7.27x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).


Section 9.15.8.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 9.15.9: Binary loop #145
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-150
In the binary file, the address of the loop is: 40801d

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 9.15.9.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 9.15.9.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 9.15.9.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.47 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 9.15.10: Binary loop #144
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-183
In the binary file, the address of the loop is: 40824b

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.15.11: Binary loop #131
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/x_solve.f:5-308
In the binary file, the address of the loop is: 407960

This loop cannot be analyzed: too many paths (12404 paths > MAX_NB_PATHS=8).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 10: Function: ninvr
===========================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/ninvr.f

Section 10.1: Source loop ending at line 37
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 158
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 158

Section 10.1.1: Binary (requested) loop #158
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/ninvr.f:5-37
In the binary file, the address of the loop is: 408d37

4% of peak computational performance is used (0.70 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 10.00 to 5.25 cycles (1.90x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.00 to 2.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 10.00 to 7.75 cycles (1.29x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 10.2: Binary loops in the function named ninvr
======================================================

Section 10.2.1: Binary loop #157
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/ninvr.f:5-37
In the binary file, the address of the loop is: 408d0e

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 10.2.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 10.2.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 10.2.2: Binary loop #156
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/ninvr.f:5-37
In the binary file, the address of the loop is: 408cd6

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 10.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 10.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 10.2.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 10.2.2.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 10.2.2.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 11: Function: y_solve
=============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/y_solve.f

Section 11.1: Source loop ending at line 46
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 183

The analysis will be displayed for the requested loops: 183

Section 11.1.1: Binary (requested) loop #183
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:40-46
In the binary file, the address of the loop is: 408f61

This loop cannot be analyzed: too many paths (27 paths > MAX_NB_PATHS=8).


Section 11.2: Source loop ending at line 54
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 182
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 182

Section 11.2.1: Binary (requested) loop #182
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:49-54
In the binary file, the address of the loop is: 409022

6% of peak computational performance is used (1.03 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 7.75 to 5.50 cycles (1.41x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.75 to 1.45 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.75 to 5.00 cycles (1.55x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.3: Source loop ending at line 71
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 180
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 180

Section 11.3.1: Binary (requested) loop #180
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-71
In the binary file, the address of the loop is: 4090fe

5% of peak computational performance is used (0.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.25 to 7.00 cycles (1.18x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.25 to 2.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 11.4: Source loop ending at line 81
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 179
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 179

Section 11.4.1: Binary (requested) loop #179
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-81
In the binary file, the address of the loop is: 409209

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.25 to 5.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.25 to 1.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 11.5: Source loop ending at line 94
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 177
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 177

Section 11.5.1: Binary (requested) loop #177
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-94
In the binary file, the address of the loop is: 4092e7

5% of peak computational performance is used (0.88 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.00 to 7.00 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.00 to 2.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 7.00 cycles (1.14x speedup).


Section 11.6: Source loop ending at line 115
============================================

Composition and unrolling
-------------------------
It is composed of the loop 176
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 176

Section 11.6.1: Binary (requested) loop #176
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-115
In the binary file, the address of the loop is: 4093f9

3% of peak computational performance is used (0.60 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 10.00 to 4.50 cycles (2.22x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.00 to 2.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.7: Source loop ending at line 140
============================================

Composition and unrolling
-------------------------
It is composed of the loop 172
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 172

Section 11.7.1: Binary (requested) loop #172
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:138-140
In the binary file, the address of the loop is: 409684

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.8: Source loop ending at line 148
============================================

Composition and unrolling
-------------------------
It is composed of the loop 173
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 173

Section 11.8.1: Binary (requested) loop #173
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:146-148
In the binary file, the address of the loop is: 409700

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.9: Source loop ending at line 174
============================================

Composition and unrolling
-------------------------
It is composed of the loop 168
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 168

Section 11.9.1: Binary (requested) loop #168
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:172-174
In the binary file, the address of the loop is: 4098ab

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.10: Source loop ending at line 181
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 169
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 169

Section 11.10.1: Binary (requested) loop #169
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:180-181
In the binary file, the address of the loop is: 4098d5

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 11.11: Source loop ending at line 225
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 167
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 167

Section 11.11.1: Binary (requested) loop #167
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-225
In the binary file, the address of the loop is: 40999f

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.12: Source loop ending at line 261
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 163
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 163

Section 11.12.1: Binary (requested) loop #163
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-261
In the binary file, the address of the loop is: 409cb2

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 19.50 cycles (2.87x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.13: Source loop ending at line 274
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 164
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 164

Section 11.13.1: Binary (requested) loop #164
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:272-274
In the binary file, the address of the loop is: 409f2a

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.14: Source loop ending at line 293
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 161
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 161

Section 11.14.1: Binary (requested) loop #161
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:290-293
In the binary file, the address of the loop is: 40a107

8% of peak computational performance is used (1.33 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 11.15: Binary loops in the function named y_solve
=========================================================

Section 11.15.1: Binary loop #170
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-181
In the binary file, the address of the loop is: 4097ba

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 13.25 cycles (2.11x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.15.2: Binary loop #166
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-225
In the binary file, the address of the loop is: 409939

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.15.2.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.75 to 1.09 cycles (7.11x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.75 to 5.00 cycles (1.55x speedup).


Section 11.15.2.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


Section 11.15.3: Binary loop #165
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-280
In the binary file, the address of the loop is: 409edd

2% of peak computational performance is used (0.44 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 9.00 to 4.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.00 to 2.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.00 to 8.00 cycles (1.12x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.15.4: Binary loop #181
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:39-54
In the binary file, the address of the loop is: 408f3b

This loop cannot be analyzed: too many paths (83 paths > MAX_NB_PATHS=8).


Section 11.15.5: Binary loop #178
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-81
In the binary file, the address of the loop is: 4091e9

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.15.5.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 11.15.5.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 11.15.6: Binary loop #162
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-304
In the binary file, the address of the loop is: 40a06c

3% of peak computational performance is used (0.54 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.75 to 6.50 cycles (2.27x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.75 to 3.69 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.75 to 13.00 cycles (1.13x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.15.7: Binary loop #160
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-304
In the binary file, the address of the loop is: 40a00a

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.15.7.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.34 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 11.15.7.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.87 cycles (6.90x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 3.75 cycles (1.60x speedup).


Section 11.15.7.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.25 to 1.03 cycles (7.03x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.25 to 5.00 cycles (1.45x speedup).


Section 11.15.8: Binary loop #175
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-115
In the binary file, the address of the loop is: 4093b6

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.15.8.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.68 cycles (6.61x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.25 cycles (1.06x speedup).


Section 11.15.8.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 11.15.9: Binary loop #174
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-148
In the binary file, the address of the loop is: 409594

5% of peak computational performance is used (0.81 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 17.25 to 14.00 cycles (1.23x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 17.25 to 7.00 cycles (2.46x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.15.10: Binary loop #159
==================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-304
In the binary file, the address of the loop is: 408e7b

This loop cannot be analyzed: too many paths (733905 paths > MAX_NB_PATHS=8).


Section 11.15.11: Binary loop #171
==================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/y_solve.f:5-148
In the binary file, the address of the loop is: 409532

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.15.11.1: Path #1
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


Section 11.15.11.2: Path #2
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.84 cycles (7.14x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 4.00 cycles (1.50x speedup).


Section 11.15.11.3: Path #3
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.50 to 1.03 cycles (7.29x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.50 to 5.00 cycles (1.50x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 12: Function: pinvr
===========================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/pinvr.f

Section 12.1: Source loop ending at line 37
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 186
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 186

Section 12.1.1: Binary (requested) loop #186
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/pinvr.f:5-37
In the binary file, the address of the loop is: 40a31f

4% of peak computational performance is used (0.70 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 10.00 to 5.50 cycles (1.82x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.00 to 2.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 10.00 to 8.00 cycles (1.25x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 12.2: Binary loops in the function named pinvr
======================================================

Section 12.2.1: Binary loop #184
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/pinvr.f:5-37
In the binary file, the address of the loop is: 40a2be

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.2.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 12.2.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 12.2.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 12.2.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).


Section 12.2.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.53 cycles (7.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 12.2.2: Binary loop #185
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/pinvr.f:5-37
In the binary file, the address of the loop is: 40a2f6

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).


Section 12.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 13: Function: z_solve
=============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/z_solve.f

Section 13.1: Source loop ending at line 46
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 211

The analysis will be displayed for the requested loops: 211

Section 13.1.1: Binary (requested) loop #211
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:40-46
In the binary file, the address of the loop is: 40a546

This loop cannot be analyzed: too many paths (27 paths > MAX_NB_PATHS=8).


Section 13.2: Source loop ending at line 54
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 210
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 210

Section 13.2.1: Binary (requested) loop #210
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:49-54
In the binary file, the address of the loop is: 40a607

6% of peak computational performance is used (1.03 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 7.75 to 5.50 cycles (1.41x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.75 to 1.45 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.75 to 5.00 cycles (1.55x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.3: Source loop ending at line 72
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 208
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 208

Section 13.3.1: Binary (requested) loop #208
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-72
In the binary file, the address of the loop is: 40a6e0

5% of peak computational performance is used (0.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.25 to 7.00 cycles (1.18x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.25 to 2.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 13.4: Source loop ending at line 81
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 207
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 207

Section 13.4.1: Binary (requested) loop #207
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-81
In the binary file, the address of the loop is: 40a7e8

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.25 to 5.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.25 to 1.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 13.5: Source loop ending at line 95
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 205
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 205

Section 13.5.1: Binary (requested) loop #205
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-95
In the binary file, the address of the loop is: 40a8c2

5% of peak computational performance is used (0.88 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.00 to 7.00 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.00 to 2.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 7.00 cycles (1.14x speedup).


Section 13.6: Source loop ending at line 117
============================================

Composition and unrolling
-------------------------
It is composed of the loop 204
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 204

Section 13.6.1: Binary (requested) loop #204
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-117
In the binary file, the address of the loop is: 40a9d0

3% of peak computational performance is used (0.60 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 10.00 to 4.50 cycles (2.22x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.00 to 2.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.7: Source loop ending at line 142
============================================

Composition and unrolling
-------------------------
It is composed of the loop 200
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 200

Section 13.7.1: Binary (requested) loop #200
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:140-142
In the binary file, the address of the loop is: 40acac

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.8: Source loop ending at line 150
============================================

Composition and unrolling
-------------------------
It is composed of the loop 201
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 201

Section 13.8.1: Binary (requested) loop #201
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:148-150
In the binary file, the address of the loop is: 40ad29

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.9: Source loop ending at line 175
============================================

Composition and unrolling
-------------------------
It is composed of the loop 196
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 196

Section 13.9.1: Binary (requested) loop #196
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:173-175
In the binary file, the address of the loop is: 40aef6

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.10: Source loop ending at line 182
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 197
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 197

Section 13.10.1: Binary (requested) loop #197
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:181-182
In the binary file, the address of the loop is: 40af20

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).


Section 13.11: Source loop ending at line 226
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 195
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 195

Section 13.11.1: Binary (requested) loop #195
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-226
In the binary file, the address of the loop is: 40b01f

6% of peak computational performance is used (1.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 30.00 to 28.00 cycles (1.07x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 30.00 to 14.00 cycles (2.14x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.12: Source loop ending at line 263
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 191
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 191

Section 13.12.1: Binary (requested) loop #191
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-263
In the binary file, the address of the loop is: 40b36e

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 21.00 cycles (2.67x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.13: Source loop ending at line 276
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 192
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 192

Section 13.13.1: Binary (requested) loop #192
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:274-276
In the binary file, the address of the loop is: 40b5e2

6% of peak computational performance is used (1.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.14: Source loop ending at line 300
=============================================

Composition and unrolling
-------------------------
It is composed of the loop 189
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 189

Section 13.14.1: Binary (requested) loop #189
=============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:297-300
In the binary file, the address of the loop is: 40b7ac

8% of peak computational performance is used (1.33 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 13.15: Binary loops in the function named z_solve
=========================================================

Section 13.15.1: Binary loop #203
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-117
In the binary file, the address of the loop is: 40a98d

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.15.1.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.68 cycles (6.61x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.25 cycles (1.06x speedup).


Section 13.15.1.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 13.15.2: Binary loop #187
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-311
In the binary file, the address of the loop is: 40a467

This loop cannot be analyzed: too many paths (733905 paths > MAX_NB_PATHS=8).


Section 13.15.3: Binary loop #198
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-182
In the binary file, the address of the loop is: 40ae01

2% of peak computational performance is used (0.39 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 13.50 cycles (2.07x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.15.4: Binary loop #199
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-150
In the binary file, the address of the loop is: 40ab0b

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.15.4.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


Section 13.15.4.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.00 to 1.75 cycles (4.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 5.25 cycles (1.52x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.15.4.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.50 to 1.75 cycles (5.43x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.50 to 8.00 cycles (1.19x speedup).


Section 13.15.5: Binary loop #194
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-226
In the binary file, the address of the loop is: 40af87

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.15.5.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 10.25 to 2.00 cycles (5.12x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 10.25 to 9.00 cycles (1.14x speedup).


Section 13.15.5.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).


Section 13.15.6: Binary loop #209
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:39-54
In the binary file, the address of the loop is: 40a520

This loop cannot be analyzed: too many paths (83 paths > MAX_NB_PATHS=8).


Section 13.15.7: Binary loop #193
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-282
In the binary file, the address of the loop is: 40b5ac

3% of peak computational performance is used (0.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.00 to 4.00 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.00 to 2.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.15.8: Binary loop #206
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-81
In the binary file, the address of the loop is: 40a7c8

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.15.8.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 13.15.8.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 13.15.9: Binary loop #190
=================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-311
In the binary file, the address of the loop is: 40b731

3% of peak computational performance is used (0.62 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 6.50 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 3.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 13.15.10: Binary loop #188
==================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-311
In the binary file, the address of the loop is: 40b6ae

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.15.10.1: Path #1
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.34 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 13.15.10.2: Path #2
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.50 to 1.28 cycles (5.85x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.50 to 5.00 cycles (1.50x speedup).


Section 13.15.10.3: Path #3
===========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 1.45 cycles (6.04x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.75 to 6.00 cycles (1.46x speedup).


Section 13.15.11: Binary loop #202
==================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/z_solve.f:5-150
In the binary file, the address of the loop is: 40ab9b

4% of peak computational performance is used (0.74 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.00 to 14.00 cycles (1.36x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 25% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.00 to 7.00 cycles (2.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 19.00 to 14.00 cycles (1.36x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 14: Function: tzetar
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/tzetar.f

Section 14.1: Source loop ending at line 51
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 214
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 214

Section 14.1.1: Binary (requested) loop #214
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/tzetar.f:5-51
In the binary file, the address of the loop is: 40b9dc

9% of peak computational performance is used (1.54 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 17.50 to 14.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 17.50 to 7.00 cycles (2.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 17.50 to 16.00 cycles (1.09x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 14.2: Binary loops in the function named tzetar
=======================================================

Section 14.2.1: Binary loop #212
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/tzetar.f:5-51
In the binary file, the address of the loop is: 40b95a

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 14.2.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.2.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.88 cycles (4.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.2.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.2.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.88 cycles (4.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.2.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 14.2.2: Binary loop #213
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/tzetar.f:5-51
In the binary file, the address of the loop is: 40b99f

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 14.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.69 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 14.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 15: Function: add
=========================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/add.f

Section 15.1: Source loop ending at line 23
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 217
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 217

Section 15.1.1: Binary (requested) loop #217
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/add.f:22-23
In the binary file, the address of the loop is: 40bc4c

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).



Section 15.2: Binary loops in the function named add
====================================================

Section 15.2.1: Binary loop #218
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/add.f:21-23
In the binary file, the address of the loop is: 40bc3a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 15.2.2: Binary loop #216
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/add.f:20-23
In the binary file, the address of the loop is: 40bc1d

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 15.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 15.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 15.2.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).


Section 15.2.3: Binary loop #215
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/add.f:19-23
In the binary file, the address of the loop is: 40bc08

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 15.2.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 15.2.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 15.2.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 15.2.3.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 15.2.3.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 15.2.3.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).



All innermost loops were analyzed.

Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 16: Function: txinvr
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/txinvr.f

Section 16.1: Source loop ending at line 48
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 221
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 221

Section 16.1.1: Binary (requested) loop #221
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/txinvr.f:5-48
In the binary file, the address of the loop is: 40bda1

9% of peak computational performance is used (1.49 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 17.50 to 14.00 cycles (1.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 17.50 to 7.00 cycles (2.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 17.50 to 16.00 cycles (1.09x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 16.2: Binary loops in the function named txinvr
=======================================================

Section 16.2.1: Binary loop #220
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/txinvr.f:5-48
In the binary file, the address of the loop is: 40bd69

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 16.2.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.63 cycles (7.16x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.50 cycles (1.29x speedup).


Section 16.2.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 16.2.2: Binary loop #219
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/txinvr.f:5-48
In the binary file, the address of the loop is: 40bd25

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 16.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 16.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.62 cycles (4.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 1.67 cycles (1.80x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 16.2.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.75 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).


Section 16.2.2.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.62 cycles (4.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 1.67 cycles (1.80x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 16.2.2.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.75 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 4.00 cycles (1.12x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 17: Function: rhs_norm
==============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/error.f

Section 17.1: Source loop ending at line 62
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 222
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 222

Section 17.1.1: Binary (requested) loop #222
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:61-62
In the binary file, the address of the loop is: 40bf74

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 17.2: Source loop ending at line 70
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 226
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 226

Section 17.2.1: Binary (requested) loop #226
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:68-70
In the binary file, the address of the loop is: 40c005

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.75 to 1.50 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.25 cycles (1.40x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 17.3: Source loop ending at line 80
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 223
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 223

Section 17.3.1: Binary (requested) loop #223
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:76-80
In the binary file, the address of the loop is: 40c05b

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 5.00 cycles (11.20x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 17.4: Binary loops in the function named rhs_norm
=========================================================

Section 17.4.1: Binary loop #224
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:65-70
In the binary file, the address of the loop is: 40bfbd

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 17.4.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 17.4.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 17.4.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 17.4.1.4: Path #4
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 17.4.1.5: Path #5
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).


Section 17.4.1.6: Path #6
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.34 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 2.00 cycles (1.12x speedup).


Section 17.4.2: Binary loop #227
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:67-70
In the binary file, the address of the loop is: 40bfef

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).


Section 17.4.3: Binary loop #225
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:66-70
In the binary file, the address of the loop is: 40bfd2

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 17.4.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.19 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Pass to your compiler a micro-architecture specialization option:
 - use version 4.6 or later and use march=native


Section 17.4.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.25 to 0.28 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.25 to 1.75 cycles (1.29x speedup).


Section 17.4.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Warning: The last block is not a loop exit
Warning: Blocks were reordered
Section 18: Function: error_norm
================================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/error.f

Section 18.1: Source loop ending at line 22
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 228
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 228

Section 18.1.1: Binary (requested) loop #228
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:21-22
In the binary file, the address of the loop is: 40c0c0

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.2: Source loop ending at line 35
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 232
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 232

Section 18.2.1: Binary (requested) loop #232
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:33-35
In the binary file, the address of the loop is: 40c1e0

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Pass to your compiler a micro-architecture specialization option:
  * use version 4.6 or later and use march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 18.3: Source loop ending at line 45
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 229
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 229

Section 18.3.1: Binary (requested) loop #229
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:41-45
In the binary file, the address of the loop is: 40c24d

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 5.00 cycles (11.20x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 18.4: Binary loops in the function named error_norm
===========================================================

Section 18.4.1: Binary loop #231
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:27-35
In the binary file, the address of the loop is: 40c14b

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.31 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 18.4.1.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.25 to 2.00 cycles (1.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.48 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 18.4.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 1.50 cycles (2.83x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.76 cycles (5.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 2.50 cycles (1.70x speedup).


Section 18.4.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.75 to 2.50 cycles (2.30x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.75 to 0.82 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.75 to 4.00 cycles (1.44x speedup).


Section 18.4.2: Binary loop #230
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:25-35
In the binary file, the address of the loop is: 40c0f3

This loop has 6 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 18.4.2.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.00 cycles (1.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.51 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 18.4.2.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.4.2.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.4.2.4: Path #4
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.50 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.4.2.5: Path #5
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 1.50 cycles (3.33x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.00 cycles (5.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.00 cycles (1.25x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.4.2.6: Path #6
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.00 to 2.50 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.12 cycles (5.33x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.50 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 18.4.3: Binary loop #233
================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/error.f:29-35
In the binary file, the address of the loop is: 40c19d

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.75 to 2.00 cycles (2.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.73 cycles (6.55x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.75 cycles (1.27x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 19: Function: verify
============================

These loops are supposed to be defined in: /home/jgarcia/TFG/source_code/SP/verify.f

Section 19.1: Source loop ending at line 37
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 234
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 234

Section 19.1.1: Binary (requested) loop #234
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/verify.f:36-37
In the binary file, the address of the loop is: 40c30f

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 1.75 cycles (8.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 19.2: Source loop ending at line 45
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 235
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 235

Section 19.2.1: Binary (requested) loop #235
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/verify.f:43-45
In the binary file, the address of the loop is: 40c345

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.3: Source loop ending at line 272
============================================

Composition and unrolling
-------------------------
It is composed of the loop 236
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 236

Section 19.3.1: Binary (requested) loop #236
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/verify.f:269-272
In the binary file, the address of the loop is: 40cae5

0% of peak computational performance is used (0.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 32% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 4.50 cycles (6.22x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 19.4: Source loop ending at line 313
============================================

Composition and unrolling
-------------------------
It is composed of the loop 237
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 237

Section 19.4.1: Binary (requested) loop #237
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/verify.f:306-313
In the binary file, the address of the loop is: 40ce21

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 19.4.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 11% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.4.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 3.50 cycles (3.71x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 11.75 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.4.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 3.50 cycles (4.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.5: Source loop ending at line 332
============================================

Composition and unrolling
-------------------------
It is composed of the loop 238
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 238

Section 19.5.1: Binary (requested) loop #238
============================================

The loop is defined in /home/jgarcia/TFG/source_code/SP/verify.f:325-332
In the binary file, the address of the loop is: 40d162

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 19.5.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 11% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.5.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 3.50 cycles (3.71x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 11.75 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 19.5.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 3.50 cycles (4.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Section 20: Function: __libc_csu_init
=====================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 20.1: Binary loops in the function named __libc_csu_init
================================================================

Section 20.1.1: Binary loop #239
================================

In the binary file, the address of the loop is: 40e150

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.00 cycles (2.00x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Section 21: Function: __do_global_ctors_aux
===========================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 21.1: Binary loops in the function named __do_global_ctors_aux
======================================================================

Section 21.1.1: Binary loop #240
================================

In the binary file, the address of the loop is: 40e1b0

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



All innermost loops were analyzed.

