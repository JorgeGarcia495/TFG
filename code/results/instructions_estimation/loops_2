Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 1: Function: bt
=======================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f

Section 1.1: Source loop ending at line 127
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 1
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 1

Section 1.1.1: Binary (requested) loop #1
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f:126-127
In the binary file, the address of the loop is: 401242

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 1.2: Source loop ending at line 141
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 2
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 2

Section 1.2.1: Binary (requested) loop #2
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f:140-141
In the binary file, the address of the loop is: 40129d

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.25 cycles (1.60x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 1.3: Source loop ending at line 153
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 5
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 5

Section 1.3.1: Binary (requested) loop #5
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f:145-153
In the binary file, the address of the loop is: 4012f4

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 1.3.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.00 cycles (1.22x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 1.3.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 9.50 cycles (1.16x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 1.3.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.66 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 3.75 cycles (1.40x speedup).


Section 1.4: Source loop ending at line 183
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 3
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 3

Section 1.4.1: Binary (requested) loop #3
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f:182-183
In the binary file, the address of the loop is: 40155b

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.00 to 1.25 cycles (2.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.50 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 1.5: Source loop ending at line 201
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 4
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 4

Section 1.5.1: Binary (requested) loop #4
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/bt.f:189-201
In the binary file, the address of the loop is: 40160b

This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.21 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 1.5.1.1: Path #1
========================

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 42.00 to 21.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 1.5.1.2: Path #2
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 1.5.1.3: Path #3
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 1.5.1.4: Path #4
========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 13.00 cycles (1.08x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 1.6: Binary loops in the function named bt
==================================================

Section 1.6.1: Binary loop #0
=============================

In the binary file, the address of the loop is: 401199

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 9% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.16 cycles (19.20x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 2: Function: initialize
===============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f

Section 2.1: Source loop ending at line 30
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 30
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 30

Section 2.1.1: Binary (requested) loop #30
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:29-30
In the binary file, the address of the loop is: 401b0f

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.2: Source loop ending at line 75
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 27
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 27

Section 2.2.1: Binary (requested) loop #27
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:65-75
In the binary file, the address of the loop is: 401d73

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 32% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.50 to 2.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 9.50 to 7.75 cycles (1.23x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.3: Source loop ending at line 97
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 22
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 22

Section 2.3.1: Binary (requested) loop #22
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:96-97
In the binary file, the address of the loop is: 401ee8

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.4: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 19
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 19

Section 2.4.1: Binary (requested) loop #19
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:113-114
In the binary file, the address of the loop is: 401fdb

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.5: Source loop ending at line 130
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 16
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 16

Section 2.5.1: Binary (requested) loop #16
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:129-130
In the binary file, the address of the loop is: 4020a9

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.6: Source loop ending at line 147
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 13
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 13

Section 2.6.1: Binary (requested) loop #13
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:146-147
In the binary file, the address of the loop is: 402198

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.7: Source loop ending at line 163
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 10
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 10

Section 2.7.1: Binary (requested) loop #10
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:162-163
In the binary file, the address of the loop is: 40226b

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.8: Source loop ending at line 179
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 7
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 7

Section 2.8.1: Binary (requested) loop #7
=========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:178-179
In the binary file, the address of the loop is: 40235f

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



Section 2.9: Binary loops in the function named initialize
==========================================================

Section 2.9.1: Binary loop #6
=============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:173-179
In the binary file, the address of the loop is: 4022d3

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.1.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.25 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.25 cycles (1.38x speedup).


Section 2.9.1.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.2: Binary loop #8
=============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:175-179
In the binary file, the address of the loop is: 402319

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.75 cycles (1.20x speedup).


Section 2.9.3: Binary loop #9
=============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:157-163
In the binary file, the address of the loop is: 4021e8

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.24 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.3.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 2.25 cycles (1.89x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.59 cycles (7.16x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 3.00 cycles (1.42x speedup).


Section 2.9.3.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.4: Binary loop #11
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:159-163
In the binary file, the address of the loop is: 402225

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.75 cycles (1.20x speedup).


Section 2.9.5: Binary loop #12
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:141-147
In the binary file, the address of the loop is: 40211a

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.27 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.5.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 2.25 cycles (1.67x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.59 cycles (6.32x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 2.75 cycles (1.36x speedup).


Section 2.9.5.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.6: Binary loop #14
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:143-147
In the binary file, the address of the loop is: 402152

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.75 cycles (1.20x speedup).


Section 2.9.7: Binary loop #15
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:124-130
In the binary file, the address of the loop is: 40202e

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.7.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.53 cycles (6.59x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).


Section 2.9.7.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.8: Binary loop #17
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:126-130
In the binary file, the address of the loop is: 402063

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.75 cycles (1.20x speedup).


Section 2.9.9: Binary loop #18
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:108-114
In the binary file, the address of the loop is: 401f4c

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.9.1: Path #1
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.53 cycles (6.59x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).


Section 2.9.9.2: Path #2
========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.10: Binary loop #20
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:110-114
In the binary file, the address of the loop is: 401f85

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.50 to 2.00 cycles (2.75x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.50 to 0.75 cycles (7.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.50 to 4.50 cycles (1.22x speedup).


Section 2.9.11: Binary loop #21
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:91-97
In the binary file, the address of the loop is: 401e5c

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.11.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.53 cycles (6.59x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).


Section 2.9.11.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).


Section 2.9.12: Binary loop #23
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:93-97
In the binary file, the address of the loop is: 401e95

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.19 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.25 to 2.00 cycles (2.62x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.25 to 0.72 cycles (7.27x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.25 to 4.25 cycles (1.24x speedup).


Section 2.9.13: Binary loop #24
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:26-30
In the binary file, the address of the loop is: 401ace

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.13.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 2.9.13.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 2.9.13.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 2.9.14: Binary loop #25
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:43-75
In the binary file, the address of the loop is: 401b4a

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.14.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.50 cycles (6.93x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 2.9.14.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.75 cycles (1.82x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.9.14.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.75 cycles (1.82x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.88 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 2.9.15: Binary loop #26
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:45-75
In the binary file, the address of the loop is: 401b8f

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

1% of peak computational performance is used (0.20 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.15.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 5.00 to 2.25 cycles (2.22x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 0.75 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 3.50 cycles (1.43x speedup).


Section 2.9.15.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.50 cycles (6.93x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 2.9.16: Binary loop #28
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:47-75
In the binary file, the address of the loop is: 401bd9

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 18.50 to 5.75 cycles (3.22x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 18.50 to 2.99 cycles (6.18x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 18.50 to 15.50 cycles (1.19x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 2.9.17: Binary loop #29
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:27-30
In the binary file, the address of the loop is: 401ae2

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 2.9.17.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 2.9.17.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 2.9.18: Binary loop #31
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:28-30
In the binary file, the address of the loop is: 401b00

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 3: Function: lhsinit
============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f

Section 3.1: Source loop ending at line 212
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 32
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 32

Section 3.1.1: Binary (requested) loop #32
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:206-212
In the binary file, the address of the loop is: 4023c6

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 2.25 cycles (2.67x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 3.2: Source loop ending at line 221
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 34
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 34

Section 3.2.1: Binary (requested) loop #34
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:219-221
In the binary file, the address of the loop is: 40241e

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.00 cycles (2.00x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



Section 3.3: Binary loops in the function named lhsinit
=======================================================

Section 3.3.1: Binary loop #33
==============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/initialize.f:205-212
In the binary file, the address of the loop is: 4023bc

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 4: Function: exact_solution
===================================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_solution.f

Section 4.1: Source loop ending at line 23
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 35
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 35

Section 4.1.1: Binary (requested) loop #35
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_solution.f:18-23
In the binary file, the address of the loop is: 402448

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.00 to 3.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 12.00 to 8.75 cycles (1.37x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 5: Function: exact_rhs
==============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f

Section 5.1: Source loop ending at line 26
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 72
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 72

Section 5.1.1: Binary (requested) loop #72
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:25-26
In the binary file, the address of the loop is: 402597

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.75 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 5.2: Source loop ending at line 45
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 67
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 67

Section 5.2.1: Binary (requested) loop #67
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:44-45
In the binary file, the address of the loop is: 4026b6

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.3: Source loop ending at line 51
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 68
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 68

Section 5.3.1: Binary (requested) loop #68
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:50-51
In the binary file, the address of the loop is: 4026e6

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.4: Source loop ending at line 93
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 66
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 66

Section 5.4.1: Binary (requested) loop #66
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:62-93
In the binary file, the address of the loop is: 40289f

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 60.00 to 15.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 60.00 to 37.75 cycles (1.59x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.5: Source loop ending at line 107
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 61
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 61

Section 5.5.1: Binary (requested) loop #61
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:100-107
In the binary file, the address of the loop is: 402c48

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 6.50 cycles (1.08x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.6: Source loop ending at line 114
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 64
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 64

Section 5.6.1: Binary (requested) loop #64
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:111-114
In the binary file, the address of the loop is: 402d27

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.7: Source loop ending at line 125
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 62
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 62

Section 5.7.1: Binary (requested) loop #62
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:118-125
In the binary file, the address of the loop is: 402df2

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 3.00 cycles (2.33x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.8: Source loop ending at line 144
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 58
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 58

Section 5.8.1: Binary (requested) loop #58
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:143-144
In the binary file, the address of the loop is: 402fc0

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.9: Source loop ending at line 150
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 59
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 59

Section 5.9.1: Binary (requested) loop #59
==========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:149-150
In the binary file, the address of the loop is: 402ff0

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.10: Source loop ending at line 191
============================================

Composition and unrolling
-------------------------
It is composed of the loop 57
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 57

Section 5.10.1: Binary (requested) loop #57
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:160-191
In the binary file, the address of the loop is: 40319f

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 60.00 to 15.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 60.00 to 37.75 cycles (1.59x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.11: Source loop ending at line 204
============================================

Composition and unrolling
-------------------------
It is composed of the loop 52
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 52

Section 5.11.1: Binary (requested) loop #52
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:197-204
In the binary file, the address of the loop is: 40353c

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 6.50 cycles (1.08x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.12: Source loop ending at line 211
============================================

Composition and unrolling
-------------------------
It is composed of the loop 55
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 55

Section 5.12.1: Binary (requested) loop #55
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:208-211
In the binary file, the address of the loop is: 403625

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.13: Source loop ending at line 222
============================================

Composition and unrolling
-------------------------
It is composed of the loop 53
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 53

Section 5.13.1: Binary (requested) loop #53
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:215-222
In the binary file, the address of the loop is: 403707

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 3.00 cycles (2.33x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.14: Source loop ending at line 242
============================================

Composition and unrolling
-------------------------
It is composed of the loop 47
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 47

Section 5.14.1: Binary (requested) loop #47
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:241-242
In the binary file, the address of the loop is: 4038dc

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.15: Source loop ending at line 248
============================================

Composition and unrolling
-------------------------
It is composed of the loop 48
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 48

Section 5.15.1: Binary (requested) loop #48
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:247-248
In the binary file, the address of the loop is: 40390c

4% of peak computational performance is used (0.67 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.50 to 1.25 cycles (1.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).


Section 5.16: Source loop ending at line 289
============================================

Composition and unrolling
-------------------------
It is composed of the loop 46
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 46

Section 5.16.1: Binary (requested) loop #46
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:258-289
In the binary file, the address of the loop is: 403abc

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 60.00 to 15.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 60.00 to 37.75 cycles (1.59x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.17: Source loop ending at line 302
============================================

Composition and unrolling
-------------------------
It is composed of the loop 41
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 41

Section 5.17.1: Binary (requested) loop #41
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:295-302
In the binary file, the address of the loop is: 403e54

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 6.50 cycles (1.08x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.18: Source loop ending at line 309
============================================

Composition and unrolling
-------------------------
It is composed of the loop 44
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 44

Section 5.18.1: Binary (requested) loop #44
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:306-309
In the binary file, the address of the loop is: 403f3d

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.75 cycles (1.05x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.19: Source loop ending at line 320
============================================

Composition and unrolling
-------------------------
It is composed of the loop 42
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 42

Section 5.19.1: Binary (requested) loop #42
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:313-320
In the binary file, the address of the loop is: 404018

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 28% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 3.00 cycles (2.33x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.20: Source loop ending at line 333
============================================

Composition and unrolling
-------------------------
It is composed of the loop 39
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 39

Section 5.20.1: Binary (requested) loop #39
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:332-333
In the binary file, the address of the loop is: 40416a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



Section 5.21: Binary loops in the function named exact_rhs
==========================================================

Section 5.21.1: Binary loop #36
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-320
In the binary file, the address of the loop is: 4037f2

This loop cannot be analyzed: too many paths (9 paths > MAX_NB_PATHS=8).


Section 5.21.2: Binary loop #37
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:329-333
In the binary file, the address of the loop is: 40411e

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.3: Binary loop #38
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:330-333
In the binary file, the address of the loop is: 404133

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 5.21.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.4: Binary loop #40
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:331-333
In the binary file, the address of the loop is: 404150

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.5: Binary loop #43
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-320
In the binary file, the address of the loop is: 403855

This loop has 8 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.09 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.5.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 22.00 to 12.50 cycles (1.76x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 22.00 to 3.88 cycles (5.68x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 22.00 to 14.50 cycles (1.52x speedup).


Section 5.21.5.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 20.00 to 11.50 cycles (1.74x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 20.00 to 3.56 cycles (5.61x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 20.00 to 13.50 cycles (1.48x speedup).


Section 5.21.5.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.75 to 6.25 cycles (2.36x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.75 to 2.28 cycles (6.47x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.75 to 10.75 cycles (1.37x speedup).


Section 5.21.5.4: Path #4
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 12.75 to 5.75 cycles (2.22x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.75 to 1.97 cycles (6.48x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 12.75 to 9.25 cycles (1.38x speedup).


Section 5.21.5.5: Path #5
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 21.75 to 12.50 cycles (1.74x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 21.75 to 3.84 cycles (5.66x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 21.75 to 14.50 cycles (1.50x speedup).


Section 5.21.5.6: Path #6
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.75 to 11.50 cycles (1.72x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.75 to 3.53 cycles (5.59x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 19.75 to 13.50 cycles (1.46x speedup).


Section 5.21.5.7: Path #7
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.50 to 6.25 cycles (2.32x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.50 to 2.25 cycles (6.44x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.50 to 10.50 cycles (1.38x speedup).


Section 5.21.5.8: Path #8
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 12.50 to 5.75 cycles (2.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.50 to 1.94 cycles (6.45x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 12.50 to 9.00 cycles (1.39x speedup).


Section 5.21.6: Binary loop #45
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:305-309
In the binary file, the address of the loop is: 403f30

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.00 cycles (1.75x speedup).


Section 5.21.7: Binary loop #49
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:237-255
In the binary file, the address of the loop is: 403884

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.21.8: Binary loop #50
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-125
In the binary file, the address of the loop is: 4025ee

This loop cannot be analyzed: too many paths (9 paths > MAX_NB_PATHS=8).


Section 5.21.9: Binary loop #51
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-222
In the binary file, the address of the loop is: 402ee0

This loop cannot be analyzed: too many paths (9 paths > MAX_NB_PATHS=8).


Section 5.21.10: Binary loop #54
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-222
In the binary file, the address of the loop is: 402f39

This loop has 8 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.09 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.10.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 22.75 to 12.50 cycles (1.82x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 22.75 to 3.97 cycles (5.73x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 22.75 to 14.50 cycles (1.57x speedup).


Section 5.21.10.2: Path #2
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 20.75 to 11.50 cycles (1.80x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 20.75 to 3.66 cycles (5.68x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 20.75 to 13.50 cycles (1.54x speedup).


Section 5.21.10.3: Path #3
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 15.50 to 6.25 cycles (2.48x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 15.50 to 2.38 cycles (6.53x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 15.50 to 11.25 cycles (1.38x speedup).


Section 5.21.10.4: Path #4
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.50 to 5.75 cycles (2.35x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.50 to 2.06 cycles (6.55x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.50 to 9.75 cycles (1.38x speedup).


Section 5.21.10.5: Path #5
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 22.50 to 12.50 cycles (1.80x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 22.50 to 3.94 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 22.50 to 14.50 cycles (1.55x speedup).


Section 5.21.10.6: Path #6
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 20.50 to 11.50 cycles (1.78x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 20.50 to 3.62 cycles (5.66x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 20.50 to 13.50 cycles (1.52x speedup).


Section 5.21.10.7: Path #7
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 15.25 to 6.25 cycles (2.44x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 15.25 to 2.34 cycles (6.51x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 15.25 to 11.00 cycles (1.39x speedup).


Section 5.21.10.8: Path #8
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.25 to 5.75 cycles (2.30x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.25 to 2.03 cycles (6.52x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.25 to 9.50 cycles (1.39x speedup).


Section 5.21.11: Binary loop #56
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:207-211
In the binary file, the address of the loop is: 403618

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.00 cycles (1.75x speedup).


Section 5.21.12: Binary loop #60
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:139-157
In the binary file, the address of the loop is: 402f68

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.21.13: Binary loop #63
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:5-125
In the binary file, the address of the loop is: 40262f

This loop has 8 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.09 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.13.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 21.75 to 12.50 cycles (1.74x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 21.75 to 3.78 cycles (5.75x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 21.75 to 13.50 cycles (1.61x speedup).


Section 5.21.13.2: Path #2
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 20.00 to 11.50 cycles (1.74x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 20.00 to 3.50 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 20.00 to 12.50 cycles (1.60x speedup).


Section 5.21.13.3: Path #3
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.75 to 6.25 cycles (2.20x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.75 to 2.09 cycles (6.57x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.75 to 10.00 cycles (1.38x speedup).


Section 5.21.13.4: Path #4
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 12.00 to 5.75 cycles (2.09x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 12.00 to 1.81 cycles (6.62x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 12.00 to 8.75 cycles (1.37x speedup).


Section 5.21.13.5: Path #5
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 21.50 to 12.50 cycles (1.72x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 21.50 to 3.75 cycles (5.73x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 21.50 to 13.50 cycles (1.59x speedup).


Section 5.21.13.6: Path #6
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 19.75 to 11.50 cycles (1.72x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 24% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 19.75 to 3.47 cycles (5.69x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 19.75 to 12.50 cycles (1.58x speedup).


Section 5.21.13.7: Path #7
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.50 to 6.25 cycles (2.16x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.50 to 2.06 cycles (6.55x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.50 to 9.75 cycles (1.38x speedup).


Section 5.21.13.8: Path #8
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 11.75 to 5.75 cycles (2.04x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.75 to 1.78 cycles (6.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.75 to 8.50 cycles (1.38x speedup).


Section 5.21.14: Binary loop #65
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:110-114
In the binary file, the address of the loop is: 402d1a

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.00 cycles (1.75x speedup).


Section 5.21.15: Binary loop #69
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:40-58
In the binary file, the address of the loop is: 40265e

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

5% of peak computational performance is used (0.93 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 26% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 11.75 cycles (1.19x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 5.21.16: Binary loop #70
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:22-26
In the binary file, the address of the loop is: 402552

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.16.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.16.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.16.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.17: Binary loop #71
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:23-26
In the binary file, the address of the loop is: 402567

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 5.21.17.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 5.21.17.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 5.21.18: Binary loop #73
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/exact_rhs.f:24-26
In the binary file, the address of the loop is: 402584

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 6: Function: compute_rhs
================================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f

Section 6.1: Source loop ending at line 33
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 129
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 129

Section 6.1.1: Binary (requested) loop #129
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:23-33
In the binary file, the address of the loop is: 404acc

5% of peak computational performance is used (0.86 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 9.25 cycles (1.51x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.2: Source loop ending at line 48
==========================================

Composition and unrolling
-------------------------
It is composed of the loop 126
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 126

Section 6.2.1: Binary (requested) loop #126
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:47-48
In the binary file, the address of the loop is: 404c17

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.83 cycles (1.20x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.3: Source loop ending at line 110
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 124
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 124

Section 6.3.1: Binary (requested) loop #124
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:61-110
In the binary file, the address of the loop is: 404e54

10% of peak computational performance is used (1.60 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 58.00 to 14.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 58.00 to 36.00 cycles (1.61x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.4: Source loop ending at line 122
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 115
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 115

Section 6.4.1: Binary (requested) loop #115
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:119-122
In the binary file, the address of the loop is: 4051a9

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.50 cycles (1.20x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.5: Source loop ending at line 129
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 116
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 116

Section 6.5.1: Binary (requested) loop #116
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:126-129
In the binary file, the address of the loop is: 4051e9

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.75 cycles (1.07x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.6: Source loop ending at line 139
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 122
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 122

Section 6.6.1: Binary (requested) loop #122
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:135-139
In the binary file, the address of the loop is: 4052ba

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.50 cycles (1.11x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.7: Source loop ending at line 149
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 119
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 119

Section 6.7.1: Binary (requested) loop #119
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:146-149
In the binary file, the address of the loop is: 40539a

11% of peak computational performance is used (1.88 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 1.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 4.00 cycles (1.06x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.8: Source loop ending at line 156
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 120
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 120

Section 6.8.1: Binary (requested) loop #120
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:153-156
In the binary file, the address of the loop is: 405401

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.9: Source loop ending at line 212
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 112
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 112

Section 6.9.1: Binary (requested) loop #112
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:168-212
In the binary file, the address of the loop is: 40571c

10% of peak computational performance is used (1.60 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 58.00 to 14.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 58.00 to 42.75 cycles (1.36x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.10: Source loop ending at line 224
============================================

Composition and unrolling
-------------------------
It is composed of the loop 107
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 107

Section 6.10.1: Binary (requested) loop #107
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:221-224
In the binary file, the address of the loop is: 405bd9

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.11: Source loop ending at line 233
============================================

Composition and unrolling
-------------------------
It is composed of the loop 109
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 109

Section 6.11.1: Binary (requested) loop #109
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:230-233
In the binary file, the address of the loop is: 405c6d

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.75 cycles (1.07x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.12: Source loop ending at line 243
============================================

Composition and unrolling
-------------------------
It is composed of the loop 105
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 105

Section 6.12.1: Binary (requested) loop #105
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:239-243
In the binary file, the address of the loop is: 405db4

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.25 cycles (1.18x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.13: Source loop ending at line 253
============================================

Composition and unrolling
-------------------------
It is composed of the loop 101
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 101

Section 6.13.1: Binary (requested) loop #101
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:250-253
In the binary file, the address of the loop is: 405ec3

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.14: Source loop ending at line 262
============================================

Composition and unrolling
-------------------------
It is composed of the loop 103
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 103

Section 6.14.1: Binary (requested) loop #103
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:259-262
In the binary file, the address of the loop is: 405f5f

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.15: Source loop ending at line 319
============================================

Composition and unrolling
-------------------------
It is composed of the loop 98
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 98

Section 6.15.1: Binary (requested) loop #98
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:274-319
In the binary file, the address of the loop is: 4061f2

10% of peak computational performance is used (1.60 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 58.00 to 14.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 58.00 to 42.50 cycles (1.36x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.16: Source loop ending at line 333
============================================

Composition and unrolling
-------------------------
It is composed of the loop 94
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 94

Section 6.16.1: Binary (requested) loop #94
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:330-333
In the binary file, the address of the loop is: 4066d1

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.00 cycles (1.50x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.17: Source loop ending at line 344
============================================

Composition and unrolling
-------------------------
It is composed of the loop 92
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 92

Section 6.17.1: Binary (requested) loop #92
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:341-344
In the binary file, the address of the loop is: 4067a8

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.75 cycles (1.07x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.18: Source loop ending at line 356
============================================

Composition and unrolling
-------------------------
It is composed of the loop 88
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 88

Section 6.18.1: Binary (requested) loop #88
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:352-356
In the binary file, the address of the loop is: 40695a

11% of peak computational performance is used (1.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 5.00 to 1.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 5.00 to 4.25 cycles (1.18x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.19: Source loop ending at line 368
============================================

Composition and unrolling
-------------------------
It is composed of the loop 85
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 85

Section 6.19.1: Binary (requested) loop #85
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:365-368
In the binary file, the address of the loop is: 406ae2

12% of peak computational performance is used (2.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 2.50 cycles (1.60x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.20: Source loop ending at line 379
============================================

Composition and unrolling
-------------------------
It is composed of the loop 83
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 83

Section 6.20.1: Binary (requested) loop #83
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:376-379
In the binary file, the address of the loop is: 406bc8

11% of peak computational performance is used (1.85 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.81 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 6.21: Source loop ending at line 389
============================================

Composition and unrolling
-------------------------
It is composed of the loop 78
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 78

Section 6.21.1: Binary (requested) loop #78
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:388-389
In the binary file, the address of the loop is: 406cc7

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



Section 6.22: Binary loops in the function named compute_rhs
============================================================

Section 6.22.1: Binary loop #74
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:21-33
In the binary file, the address of the loop is: 404a82

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.38 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.50 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.1.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.50 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.2: Binary loop #75
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:44-48
In the binary file, the address of the loop is: 404bca

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.3: Binary loop #76
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:385-389
In the binary file, the address of the loop is: 406c7b

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.4: Binary loop #77
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:386-389
In the binary file, the address of the loop is: 406c90

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.4.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 6.22.4.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.5: Binary loop #79
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:387-389
In the binary file, the address of the loop is: 406cad

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.6: Binary loop #80
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:349-356
In the binary file, the address of the loop is: 406876

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.6.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.38 cycles (6.67x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 2.00 cycles (1.25x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.6.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.62 cycles (4.31x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 6.25 cycles (1.12x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.6.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.00 to 1.62 cycles (4.31x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.00 to 6.25 cycles (1.12x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.7: Binary loop #81
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:363-368
In the binary file, the address of the loop is: 406a73

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.7.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.25 cycles (1.23x speedup).


Section 6.22.7.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)

Workaround(s):
 - Read less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.8: Binary loop #82
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:374-379
In the binary file, the address of the loop is: 406b64

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.8.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.47 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.00 cycles (1.25x speedup).


Section 6.22.8.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)

Workaround(s):
 - Read less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.9: Binary loop #84
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:375-379
In the binary file, the address of the loop is: 406b9d

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 6.22.10: Binary loop #86
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:364-368
In the binary file, the address of the loop is: 406ab0

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.11: Binary loop #87
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:350-356
In the binary file, the address of the loop is: 4068d6

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.11.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.75 to 0.59 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.75 to 3.50 cycles (1.36x speedup).


Section 6.22.11.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)

Workaround(s):
 - Read less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.12: Binary loop #89
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:351-356
In the binary file, the address of the loop is: 406921

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.75 cycles (1.07x speedup).


Section 6.22.13: Binary loop #90
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:328-333
In the binary file, the address of the loop is: 406672

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.13.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.41 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 6.22.13.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.14: Binary loop #91
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:339-344
In the binary file, the address of the loop is: 40673a

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.14.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.44 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.14.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.15: Binary loop #93
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:340-344
In the binary file, the address of the loop is: 406776

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.16: Binary loop #95
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:329-333
In the binary file, the address of the loop is: 4066a6

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 6.22.17: Binary loop #96
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:272-319
In the binary file, the address of the loop is: 4060f7

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.17.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.43 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.17.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.00 to 2.00 cycles (4.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.00 to 6.00 cycles (1.50x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.17.3: Path #3
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.00 to 2.00 cycles (4.50x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.00 to 6.00 cycles (1.50x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.18: Binary loop #97
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:273-319
In the binary file, the address of the loop is: 406171

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.15 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.18.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 6.75 to 4.00 cycles (1.69x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 19% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.75 to 0.97 cycles (6.97x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.75 to 6.00 cycles (1.12x speedup).


Section 6.22.18.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.50 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.19: Binary loop #99
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:166-262
In the binary file, the address of the loop is: 40561a

This loop cannot be analyzed: too many paths (30 paths > MAX_NB_PATHS=8).


Section 6.22.20: Binary loop #100
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:237-243
In the binary file, the address of the loop is: 405d14

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.20.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 7.25 to 1.11 cycles (6.53x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 7.25 to 5.75 cycles (1.26x speedup).


Section 6.22.20.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.25 cycles (6.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck)
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 1.50 to 1.00 cycles (1.50x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.21: Binary loop #102
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:249-253
In the binary file, the address of the loop is: 405e91

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.22: Binary loop #104
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:258-262
In the binary file, the address of the loop is: 405f34

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 6.22.23: Binary loop #106
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:238-243
In the binary file, the address of the loop is: 405d7b

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 1.00 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.00 to 3.75 cycles (1.07x speedup).


Section 6.22.24: Binary loop #108
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:220-224
In the binary file, the address of the loop is: 405bae

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.00 to 0.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.00 to 2.75 cycles (1.09x speedup).


Section 6.22.25: Binary loop #110
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:229-233
In the binary file, the address of the loop is: 405c3b

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.88 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.26: Binary loop #111
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:167-212
In the binary file, the address of the loop is: 405696

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.11 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.26.1: Path #1
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 9.00 to 4.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.00 to 1.64 cycles (5.49x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.00 to 6.25 cycles (1.44x speedup).


Section 6.22.26.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.62 cycles (6.40x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.27: Binary loop #113
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:59-156
In the binary file, the address of the loop is: 404dbf

This loop has 5 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.27.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - reading data from caches/RAM (load units are a bottleneck)
 - writing data to caches/RAM (the store unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Read less array elements
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 6.22.27.2: Path #2
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 6.50 cycles (1.35x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 1.56 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.27.3: Path #3
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 6.50 cycles (1.35x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 1.56 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.27.4: Path #4
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 6.50 cycles (1.35x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 1.56 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.27.5: Path #5
==========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 8.75 to 6.50 cycles (1.35x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 1.56 cycles (5.60x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.28: Binary loop #114
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:60-110
In the binary file, the address of the loop is: 404e06

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.17 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.28.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.87 cycles (6.91x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.28.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.75 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native


Section 6.22.29: Binary loop #117
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:117-129
In the binary file, the address of the loop is: 405182

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.25 cycles (1.22x speedup).


Section 6.22.30: Binary loop #118
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:133-139
In the binary file, the address of the loop is: 405282

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.30.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.44 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.25 cycles (1.08x speedup).


Section 6.22.30.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.31: Binary loop #121
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:144-156
In the binary file, the address of the loop is: 40534c

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.25 to 1.56 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.25 to 5.00 cycles (1.25x speedup).


Section 6.22.32: Binary loop #123
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:134-139
In the binary file, the address of the loop is: 4052b6

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.25 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.00 to 0.50 cycles (2.00x speedup).


Section 6.22.33: Binary loop #125
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:45-48
In the binary file, the address of the loop is: 404bdf

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.33.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 6.22.33.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.34: Binary loop #127
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:46-48
In the binary file, the address of the loop is: 404bfd

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 6.22.35: Binary loop #128
=================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/rhs.f:22-33
In the binary file, the address of the loop is: 404aa2

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 6.22.35.1: Path #1
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.25 to 0.44 cycles (7.47x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.25 to 3.00 cycles (1.08x speedup).


Section 6.22.35.2: Path #2
==========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 7: Function: x_solve
============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f

Section 7.1: Source loop ending at line 128
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 137
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 137

Section 7.1.1: Binary (requested) loop #137
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:46-128
In the binary file, the address of the loop is: 406f39

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 31% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 42.50 cycles (1.18x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.2: Source loop ending at line 298
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 136
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 136

Section 7.2.1: Binary (requested) loop #136
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:135-298
In the binary file, the address of the loop is: 40747e

11% of peak computational performance is used (1.84 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 106.00 to 28.06 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 106.00 to 97.50 cycles (1.09x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 7.3: Source loop ending at line 353
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 135
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 135

Section 7.3.1: Binary (requested) loop #135
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:331-353
In the binary file, the address of the loop is: 407f9b

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.25 to 2.06 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.25 to 5.00 cycles (1.65x speedup).


Section 7.4: Source loop ending at line 388
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 132
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 132

Section 7.4.1: Binary (requested) loop #132
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:386-388
In the binary file, the address of the loop is: 408132

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 7.5: Binary loops in the function named x_solve
=======================================================

Section 7.5.1: Binary loop #133
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:385-388
In the binary file, the address of the loop is: 408125

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 7.5.2: Binary loop #130
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 406e2a

This loop cannot be analyzed: too many paths (17 paths > MAX_NB_PATHS=8).


Section 7.5.3: Binary loop #131
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:5-388
In the binary file, the address of the loop is: 406e67

This loop cannot be analyzed: too many paths (16 paths > MAX_NB_PATHS=8).


Section 7.5.4: Binary loop #134
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/x_solve.f:384-388
In the binary file, the address of the loop is: 4080ee

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 22% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.61 cycles (6.18x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.75 to 3.50 cycles (1.07x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 8: Function: y_solve
============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f

Section 8.1: Source loop ending at line 126
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 145
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 145

Section 8.1.1: Binary (requested) loop #145
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:44-126
In the binary file, the address of the loop is: 40831e

7% of peak computational performance is used (1.18 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 50.00 to 12.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 50.00 to 44.00 cycles (1.14x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.2: Source loop ending at line 297
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 144
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 144

Section 8.2.1: Binary (requested) loop #144
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:134-297
In the binary file, the address of the loop is: 408863

11% of peak computational performance is used (1.84 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 106.00 to 28.06 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 106.00 to 97.50 cycles (1.09x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 8.3: Source loop ending at line 349
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 143
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 143

Section 8.3.1: Binary (requested) loop #143
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:326-349
In the binary file, the address of the loop is: 409381

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 9.50 to 2.38 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 9.50 to 6.00 cycles (1.58x speedup).


Section 8.4: Source loop ending at line 387
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 140
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 140

Section 8.4.1: Binary (requested) loop #140
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:385-387
In the binary file, the address of the loop is: 40957f

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 8.5: Binary loops in the function named y_solve
=======================================================

Section 8.5.1: Binary loop #141
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:384-387
In the binary file, the address of the loop is: 409572

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 8.5.2: Binary loop #142
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:383-387
In the binary file, the address of the loop is: 40951d

Warnings:
get_nb_fused_uops:
 - The number of fused uops of the instruction [CLTQ] is unknown

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 0.95 cycles (6.30x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 6.00 to 5.25 cycles (1.14x speedup).


Section 8.5.3: Binary loop #138
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 408231

This loop cannot be analyzed: too many paths (17 paths > MAX_NB_PATHS=8).


Section 8.5.4: Binary loop #139
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/y_solve.f:4-387
In the binary file, the address of the loop is: 40826c

This loop cannot be analyzed: too many paths (16 paths > MAX_NB_PATHS=8).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 9: Function: z_solve
============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f

Section 9.1: Source loop ending at line 126
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 153
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 153

Section 9.1.1: Binary (requested) loop #153
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:44-126
In the binary file, the address of the loop is: 40baac

7% of peak computational performance is used (1.20 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 30% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 51.00 to 12.75 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 51.00 to 45.00 cycles (1.13x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.2: Source loop ending at line 298
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 152
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 152

Section 9.2.1: Binary (requested) loop #152
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:135-298
In the binary file, the address of the loop is: 40c00e

11% of peak computational performance is used (1.84 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 33% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 106.00 to 28.06 cycles (3.78x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 106.00 to 97.50 cycles (1.09x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 9.3: Source loop ending at line 356
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 151
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 151

Section 9.3.1: Binary (requested) loop #151
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:332-356
In the binary file, the address of the loop is: 40cb2e

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 8.75 to 2.19 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 8.75 to 5.50 cycles (1.59x speedup).


Section 9.4: Source loop ending at line 400
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 148
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 148

Section 9.4.1: Binary (requested) loop #148
===========================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:398-400
In the binary file, the address of the loop is: 40cd0d

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.50 to 0.62 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.50 to 1.75 cycles (1.43x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).



Section 9.5: Binary loops in the function named z_solve
=======================================================

Section 9.5.1: Binary loop #147
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40b9ee

This loop cannot be analyzed: too many paths (16 paths > MAX_NB_PATHS=8).


Section 9.5.2: Binary loop #146
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:4-400
In the binary file, the address of the loop is: 40b9b2

This loop cannot be analyzed: too many paths (17 paths > MAX_NB_PATHS=8).


Section 9.5.3: Binary loop #149
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:397-400
In the binary file, the address of the loop is: 40cd00

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 9.5.4: Binary loop #150
===============================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/z_solve.f:396-400
In the binary file, the address of the loop is: 40ccb8

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 6.00 to 1.05 cycles (5.71x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Detected a non usual bottleneck.

Workaround(s):
Found micro-architecture specialization compiler flags: -march=x86-64:
 - Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 10: Function: add
=========================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/add.f

Section 10.1: Source loop ending at line 22
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 156
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 156

Section 10.1.1: Binary (requested) loop #156
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/add.f:21-22
In the binary file, the address of the loop is: 40ce5b

5% of peak computational performance is used (0.80 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.31 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.25 to 1.00 cycles (1.25x speedup).



Section 10.2: Binary loops in the function named add
====================================================

Section 10.2.1: Binary loop #157
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/add.f:20-22
In the binary file, the address of the loop is: 40ce41

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (no SSE/AVX instruction: only general purpose registers are used and data elements are processed one at a time).


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 10.2.2: Binary loop #155
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/add.f:19-22
In the binary file, the address of the loop is: 40ce23

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 10.2.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 10.2.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 10.2.3: Binary loop #154
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/add.f:18-22
In the binary file, the address of the loop is: 40ce0e

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 10.2.3.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 10.2.3.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 10.2.3.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 11: Function: error_norm
================================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f

Section 11.1: Source loop ending at line 33
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 161
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 161

Section 11.1.1: Binary (requested) loop #161
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:31-33
In the binary file, the address of the loop is: 40cfca

9% of peak computational performance is used (1.50 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by:
 - execution of FP add operations (the FP add unit is a bottleneck)
 - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.75 cycles (1.14x speedup).

Workaround(s):
 - Reduce the number of FP add instructions
 - Reduce the number of FP multiply/FMA instructions


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 11.2: Source loop ending at line 43
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 158
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 158

Section 11.2.1: Binary (requested) loop #158
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:39-43
In the binary file, the address of the loop is: 40d053

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 4.00 cycles (14.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 11.3: Binary loops in the function named error_norm
===========================================================

Section 11.3.1: Binary loop #159
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:23-33
In the binary file, the address of the loop is: 40cf03

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.29 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.3.1.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.25 cycles (1.56x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.50 cycles (6.93x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 3.50 to 3.00 cycles (1.17x speedup).


Section 11.3.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 2.25 cycles (1.89x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.66 cycles (6.48x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 3.00 cycles (1.42x speedup).


Section 11.3.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.25 to 2.25 cycles (1.89x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.25 to 0.66 cycles (6.48x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.25 to 3.00 cycles (1.42x speedup).


Section 11.3.2: Binary loop #162
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:27-33
In the binary file, the address of the loop is: 40cf87

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.00 cycles (2.25x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 27% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.80x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.75 cycles (1.20x speedup).


Section 11.3.3: Binary loop #160
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:25-33
In the binary file, the address of the loop is: 40cf43

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

1% of peak computational performance is used (0.22 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 11.3.3.1: Path #1
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 2.25 cycles (2.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 23% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.66 cycles (6.86x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 4.50 to 3.25 cycles (1.38x speedup).


Section 11.3.3.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 2.75 to 2.00 cycles (1.38x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 21% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.41 cycles (6.77x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 1.75 cycles (1.57x speedup).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 12: Function: rhs_norm
==============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f

Section 12.1: Source loop ending at line 72
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 166
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 166

Section 12.1.1: Binary (requested) loop #166
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:70-72
In the binary file, the address of the loop is: 40d12b

7% of peak computational performance is used (1.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 1.75 to 1.50 cycles (1.17x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 1.75 to 1.25 cycles (1.40x speedup).


FMA
---
Presence of both ADD/SUB and MUL operations.
Workaround(s):
 - Found micro-architecture specialization compiler flags: -march=x86-64:
  * Check match with analysis target. Ex: for Haswell, you should compile with -march=haswell or, on a Haswell machine, with -march=native
 - Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.
For instance a + b*c is a valid FMA (MUL then ADD).
However (a+b)* c cannot be translated into an FMA (ADD then MUL).


Section 12.2: Source loop ending at line 82
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 163
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 163

Section 12.2.1: Binary (requested) loop #163
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:78-82
In the binary file, the address of the loop is: 40d1a3

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 56.00 to 28.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 56.00 to 4.00 cycles (14.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.



Section 12.3: Binary loops in the function named rhs_norm
=========================================================

Section 12.3.1: Binary loop #165
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:68-72
In the binary file, the address of the loop is: 40d0fa

The structure of this loop is probably <if then [else] end>.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.3.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.75 to 0.37 cycles (7.38x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.75 to 2.50 cycles (1.10x speedup).


Section 12.3.1.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 12.3.2: Binary loop #164
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:67-72
In the binary file, the address of the loop is: 40d0e5

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 12.3.2.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
8 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.00 to 0.12 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 12.3.2.2: Path #2
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 12.3.2.3: Path #3
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 18% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).


Section 12.3.3: Binary loop #167
================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/error.f:69-72
In the binary file, the address of the loop is: 40d118

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.44 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).



All innermost loops were analyzed.

Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Limiting unroll analysis to the first path of a multi-paths loop
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Warning: Blocks were reordered
Section 13: Function: verify
============================

These loops are supposed to be defined in: /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f

Section 13.1: Source loop ending at line 37
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 168
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 168

Section 13.1.1: Binary (requested) loop #168
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f:36-37
In the binary file, the address of the loop is: 40d23a

0% of peak computational performance is used (0.07 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 7.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 1.75 cycles (8.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 13.2: Source loop ending at line 46
===========================================

Composition and unrolling
-------------------------
It is composed of the loop 169
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 169

Section 13.2.1: Binary (requested) loop #169
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f:44-46
In the binary file, the address of the loop is: 40d276

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
4 data elements could be processed at once in vector registers.


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.50 cycles (4.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.50 cycles (1.33x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.3: Source loop ending at line 274
============================================

Composition and unrolling
-------------------------
It is composed of the loop 170
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 170

Section 13.3.1: Binary (requested) loop #170
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f:271-274
In the binary file, the address of the loop is: 40d97a

0% of peak computational performance is used (0.14 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is probably not vectorized (store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 29% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is probably NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 28.00 to 14.00 cycles (2.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 28.00 to 4.00 cycles (7.00x speedup).

Workaround(s):
Reduce the number of division or square root instructions.
If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast.
Check whether you really need double precision. If not, switch to single precision to speedup execution.


Section 13.4: Source loop ending at line 315
============================================

Composition and unrolling
-------------------------
It is composed of the loop 171
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 171

Section 13.4.1: Binary (requested) loop #171
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f:308-315
In the binary file, the address of the loop is: 40dbfb

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.4.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 2.00 cycles (6.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 12.00 cycles (1.08x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.4.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 2.00 cycles (7.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 12.00 cycles (1.17x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.5: Source loop ending at line 334
============================================

Composition and unrolling
-------------------------
It is composed of the loop 172
and is not unrolled or unrolled with no peel/tail loop.
The analysis will be displayed for the requested loops: 172

Section 13.5.1: Binary (requested) loop #172
============================================

The loop is defined in /home/jcsalinas/Documents/pruebas/inst_estimation_module/BT/verify.f:327-334
In the binary file, the address of the loop is: 40de99

This loop has 3 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX


Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Section 13.5.1.1: Path #1
=========================

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 15% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 11.00 to 1.00 cycles (11.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 11.00 to 7.50 cycles (1.47x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.5.1.2: Path #2
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 13.00 to 2.00 cycles (6.50x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 17% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 13.00 to 1.06 cycles (12.24x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 13.00 to 12.00 cycles (1.08x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop


Section 13.5.1.3: Path #3
=========================

Code clean check
----------------
Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 14.00 to 2.00 cycles (7.00x speedup).
Workaround(s):
To reference allocatable arrays, use "allocatable" instead of "pointer" pointers or qualify them with the "contiguous" attribute (Fortran 2008).
For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop.

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 16% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 14.00 to 1.19 cycles (11.79x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one:
  * if not already done, recompile with O3 to enable the compiler vectorizer. In case of reduction loop, use Ofast instead of O3 or add ffast-math.
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:
Fortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):
do i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)


Bottlenecks
-----------
Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 14.00 to 12.00 cycles (1.17x speedup).

Workaround(s):
 - Write less array elements
 - Provide more information to your compiler:
  * hardcode the bounds of the corresponding 'for' loop



All innermost loops were analyzed.

Warning: Blocks were reordered
Section 14: Function: __libc_csu_init
=====================================

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Section 14.1: Binary loops in the function named __libc_csu_init
================================================================

Section 14.1.1: Binary loop #173
================================

In the binary file, the address of the loop is: 40eb40

Found no debug data for this function.
With GNU or Intel compilers, please recompile with -g.
With an Intel compiler you must explicitly specify an optimization level and you can prevent CQA from suggesting already used flags by adding -sox.
Alternatively, try to:
 - recompile with -debug noinline-debug-info (if using Intel compiler 13)
 - analyze the caller function (possible inlining)

Warnings:
get_path_cqa_results:
 - Detected a function call instruction: ignoring called function instructions

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

Vectorization status
--------------------
Your loop is not vectorized (all SSE/AVX instructions are used in scalar version (process only one data element in vector registers)).
Only 20% of vector register length is used (average across all SSE/AVX instructions).


Vectorization
-------------
Your loop is NOT VECTORIZED and could benefit from vectorization.
By vectorizing your loop, you can lower the cost of an iteration from 2.00 to 0.25 cycles (8.00x speedup).
Since your execution units are vector units, only a vectorized loop can use their full power.

Workaround(s):
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)


Bottlenecks
-----------
Performance is limited by instruction throughput (loading/decoding program instructions to execution core) (front-end is a bottleneck).

By removing all these bottlenecks, you can lower the cost of an iteration from 2.00 to 1.00 cycles (2.00x speedup).



All innermost loops were analyzed.

